{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "import tensorflow as tf\n",
    "import yaml\n",
    "import argparse\n",
    "\n",
    "from keras.callbacks import TensorBoard\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "from libs.FLDigitalTwin import FLDigitalTwin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--dataset\", type=str, default=\"traffic\")\n",
    "parser.add_argument(\"--prefix\", type=str, default=\"normal\")\n",
    "parser.add_argument(\"--percent_mc\", type=str, default=\"01\")\n",
    "parser.add_argument(\"--missing_mode\", type=str, default=\"noadjacency\")\n",
    "parser.add_argument(\"--matrix_ml\", type=str, default=\"10x10\")\n",
    "parser.add_argument(\"--weight_mechanism\", type=int, default=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get values defined from CLI\n",
    "CONFIG_FILE = '.config_ipynb'\n",
    "if os.path.isfile(CONFIG_FILE):\n",
    "    with open(CONFIG_FILE) as f:\n",
    "        _args = f.read().split()\n",
    "        _args = _args[1:]\n",
    "else:\n",
    "    _args = ['--dataset', 'electricity', '--prefix', 'avg', '--percent_mc', '01', '--missing_mode', 'noadjacency']\n",
    "\n",
    "args = parser.parse_args(_args)\n",
    "\n",
    "# Get configs from yaml\n",
    "with open('config.yaml', 'r') as file:\n",
    "    yaml_data = yaml.load(file, Loader=yaml.SafeLoader)\n",
    "\n",
    "config = {\n",
    "    'DATASET': args.dataset if args.dataset else yaml_data['DATASET'],\n",
    "    'NUM_DATA_SHEETS': yaml_data['NUM_DATA_SHEETS'],\n",
    "    'NUM_CLIENTS': yaml_data['NUM_CLIENTS'],\n",
    "    \"LOOK_BACK\": yaml_data['LOOK_BACK'],\n",
    "    'EPOCHS': yaml_data['EPOCHS'],\n",
    "    'CLIENT_EPOCHS': yaml_data['CLIENT_EPOCHS'],\n",
    "    'BATCH_SIZE': yaml_data['BATCH_SIZE'],\n",
    "    'SAVE_INTERVAL': yaml_data['SAVE_INTERVAL'],\n",
    "    'LEARNING_RATE': yaml_data['LEARNING_RATE'],\n",
    "    'TRAIN_SIZE': yaml_data['TRAIN_SIZE'],\n",
    "    'DATA_DIR': yaml_data['DATA_DIR'],\n",
    "    'CLIENT_MATRIX_DIR': yaml_data['CLIENT_MATRIX_DIR'],\n",
    "    'FL_OUTPUT_DIR': yaml_data['FL_OUTPUT_DIR'],\n",
    "    'GENERAL_OUTPUT_DIR': yaml_data['GENERAL_OUTPUT_DIR'],\n",
    "    'PREFIX': args.prefix if args.prefix else yaml_data['PREFIX'],\n",
    "    'PERCENTAGE_MISSING_CLIENT': args.percent_mc if args.percent_mc else yaml_data['PERCENTAGE_MISSING_CLIENT'],\n",
    "    'MISSING_MODE': args.missing_mode if args.missing_mode else yaml_data['MISSING_MODE'],\n",
    "    'MATRIX_MISSING_LENGTH': args.matrix_ml if args.matrix_ml else yaml_data['MATRIX_MISSING_LENGTH'],\n",
    "    'WEIGHT_MECHANISM': args.weight_mechanism if args.weight_mechanism else 0,\n",
    "    'WEIGHT_TRACKING_DIR': os.path.join('model_weight_track', args.dataset if args.dataset else yaml_data['DATASET'], args.prefix if args.prefix else yaml_data['PREFIX'], args.matrix_ml if args.matrix_ml else yaml_data['MATRIX_MISSING_LENGTH'], args.missing_mode if args.missing_mode else yaml_data['MISSING_MODE'])\n",
    "}\n",
    "\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Set tf random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(3)  # Set random seed for reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Initialize FL-DT Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FLDigitalTwin = FLDigitalTwin(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5.1: Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the file path and sheet names with their corresponding keys\n",
    "file_path = f\"{config['DATA_DIR']}/{config['DATASET']}/standard_{config['DATASET']}.xlsx\"\n",
    "sheets = {}\n",
    "for i in range(config['NUM_DATA_SHEETS']):\n",
    "    sheets[f'client_{str(i)}'] = f'Sheet_{str(i + 2)}'\n",
    "\n",
    "# Load the data\n",
    "dictionary = FLDigitalTwin.load_data(file_path, sheets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5.2: Example for client_1 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_name = \"client_1\"\n",
    "df = dictionary[client_name]\n",
    "df[\"Daily Value\"] = df.iloc[:, 1:-1].sum(axis=1)\n",
    "\n",
    "# Visualize data\n",
    "FLDigitalTwin.visualize_data(df, \"Energy Distribution (kWh)\", \"Daily Value (kWh)\", \"Frequency\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5.3: Split train/test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale and split data\n",
    "training_set, test_data, sc_X = FLDigitalTwin.scale_split_datasets(\n",
    "    df[\"Daily Value\"], config['TRAIN_SIZE'], config['LOOK_BACK']\n",
    ")\n",
    "x_train, y_train = FLDigitalTwin.create_rnn_dataset(training_set, config['LOOK_BACK'])\n",
    "x_test, y_test = FLDigitalTwin.create_rnn_dataset(test_data, config['LOOK_BACK'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5.4: Create models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train the model\n",
    "ts_model = FLDigitalTwin.create_model(input_shape=(1, config['LOOK_BACK']), output_shape=1)\n",
    "log_dir = f\"logs/{config['PREFIX']}fit/\" + dt.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "# with tf.device('/GPU:0'):\n",
    "ts_model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    epochs=config['CLIENT_EPOCHS'],\n",
    "    batch_size=config['BATCH_SIZE'],\n",
    "    verbose=1,\n",
    "    callbacks=[tensorboard_callback],\n",
    ")\n",
    "\n",
    "# Evaluate model\n",
    "ts_model.evaluate(x_test, y_test, verbose=1)\n",
    "predict_on_train = ts_model.predict(x_train)\n",
    "predict_on_test = ts_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5.5: Try plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_on_train = sc_X.inverse_transform(predict_on_train)\n",
    "predict_on_test = sc_X.inverse_transform(predict_on_test)\n",
    "\n",
    "# Plot predictions\n",
    "plot_original, plot_train, plot_test = FLDigitalTwin.plot_data_preparation(\n",
    "    df[\"Daily Value\"], predict_on_train, predict_on_test, config['LOOK_BACK']\n",
    ")\n",
    "FLDigitalTwin.plot_the_data(plot_original, plot_train, plot_test, \"Model Predictions vs Actual\")\n",
    "\n",
    "# Seasonal decomposition\n",
    "result = seasonal_decompose(df[\"Daily Value\"], model=\"additive\", period=365)\n",
    "result.plot()\n",
    "plt.suptitle(\"Seasonal Decomposition of Daily Value\")\n",
    "plt.show()\n",
    "\n",
    "# Enhanced visualization of seasonal decomposition\n",
    "fig, axes = plt.subplots(4, 1, figsize=(15, 12), sharex=True)\n",
    "result.observed.plot(ax=axes[0], title=\"Observed\")\n",
    "result.trend.plot(ax=axes[1], title=\"Trend\")\n",
    "result.seasonal.plot(ax=axes[2], title=\"Seasonal\")\n",
    "result.resid.plot(ax=axes[3], title=\"Residual\")\n",
    "for ax in axes:\n",
    "    ax.set_ylabel(\"Value Summation\")\n",
    "axes[3].set_xlabel(\"Time\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5.6: Try saving to excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the residuals to an Excel file\n",
    "os.makedirs(os.path.join(config['GENERAL_OUTPUT_DIR'], client_name, config['PREFIX']), exist_ok=True)\n",
    "pd.DataFrame(result.resid).to_excel(\n",
    "    os.path.join(config['GENERAL_OUTPUT_DIR'], client_name, config['PREFIX'], '_client.xlsx'), sheet_name=\"Decomposition_Residuals\"\n",
    ")\n",
    "\n",
    "# Summary statistics\n",
    "residual_stats = pd.DataFrame(\n",
    "    {\n",
    "        \"Mean\": [result.resid.mean()],\n",
    "        \"Median\": [result.resid.median()],\n",
    "        \"Standard Deviation\": [result.resid.std()],\n",
    "        \"Max\": [result.resid.max()],\n",
    "        \"Min\": [result.resid.min()],\n",
    "    }\n",
    ")\n",
    "\n",
    "# Save summary statistics\n",
    "residual_stats.to_excel(os.path.join(config['GENERAL_OUTPUT_DIR'], client_name, config['PREFIX'], 'residual_statistics.xlsx'), index=False)\n",
    "\n",
    "# Display summary statistics\n",
    "print(\"Summary Statistics of Residuals:\")\n",
    "print(residual_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5.7: Evaluate models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation metrics\n",
    "metrics = ts_model.evaluate(x_test, y_test, verbose=1)\n",
    "metrics_df = pd.DataFrame([metrics], columns=[\"Loss\", \"MSE\", \"MAE\", \"MAPE\"])\n",
    "\n",
    "# Save evaluation metrics\n",
    "metrics_df.to_excel(os.path.join(config['GENERAL_OUTPUT_DIR'], client_name, config['PREFIX'], 'evaluation_metrics.xlsx'), index=False)\n",
    "\n",
    "# Display evaluation metrics\n",
    "print(\"Evaluation Metrics:\")\n",
    "print(metrics_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5.8: Create train/test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(config)\n",
    "train_test_dataset = []\n",
    "for i in range(config['NUM_DATA_SHEETS']):\n",
    "    _x_train, _y_train, _x_test, _y_test, _sc_cl = FLDigitalTwin.create_train_test_dataset(dictionary[f'client_{str(i)}'], config['LOOK_BACK'])\n",
    "    train_test_dataset.append((_x_train, _y_train, _x_test, _y_test, _sc_cl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5.9: Create model arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_arr = []\n",
    "for i in range(config['NUM_CLIENTS']):\n",
    "    _model = FLDigitalTwin.create_model(input_shape=(1, config['LOOK_BACK']), output_shape=1)\n",
    "    model_arr.append(_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5.10: Train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(config['NUM_CLIENTS']):\n",
    "    _x_train, _y_train, _, _, _ = train_test_dataset[i]\n",
    "    FLDigitalTwin.train_model(model_arr[i], _x_train, _y_train, os.path.join('logs', 'fit', config['PREFIX'], str(i)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5.11: Evaluate models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(config['NUM_CLIENTS']):\n",
    "    _x_train, _y_train, _x_test, _y_test, _ = train_test_dataset[i]\n",
    "    FLDigitalTwin.evaluate_model(model_arr[i], _x_train, _y_train, _x_test, _y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Execute the full client update scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_matrix = np.loadtxt(os.path.join(config['CLIENT_MATRIX_DIR'], 'client_matrix_' + config['MATRIX_MISSING_LENGTH'] + '_' + config['PERCENTAGE_MISSING_CLIENT'] + '_' + config['MISSING_MODE'] + '.csv'), delimiter=',', dtype=str)\n",
    "print(client_matrix)\n",
    "\n",
    "if not os.path.exists(config['WEIGHT_TRACKING_DIR']) and config['PREFIX'] == 'weight':\n",
    "    os.makedirs(config['WEIGHT_TRACKING_DIR'])\n",
    "\n",
    "history_client_normal_dict = {}\n",
    "for i in range(len(model_arr)):\n",
    "    _x_train, _y_train, _x_test, _y_test, _ = train_test_dataset[i]\n",
    "    print(f\"Processing dataset {i}...\")\n",
    "    if args.prefix == 'normal':\n",
    "        print(\"Run with FedNormal...\")\n",
    "        hist_dict_normal = FLDigitalTwin.train_fl_full_updates(model_arr, _x_train, _y_train, _x_test, _y_test)\n",
    "    else:\n",
    "        if args.prefix == 'weight':\n",
    "            print(\"Run with FedWeightDT...\")\n",
    "            hist_dict_normal = FLDigitalTwin.train_fl_digital_twin(model_arr, _x_train, _y_train, _x_test, _y_test, client_matrix, has_weights_mechanism=True)\n",
    "        else:\n",
    "            print(\"Run with FedAvg...\")\n",
    "            hist_dict_normal = FLDigitalTwin.train_fl_digital_twin(model_arr, _x_train, _y_train, _x_test, _y_test, client_matrix, has_weights_mechanism=False)\n",
    "    \n",
    "    history_client_normal_dict[f'client_{str(i)}'] = hist_dict_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_client_save_dir = os.path.join('model_history', config['DATASET'], config['PREFIX'], 'clients', config['MATRIX_MISSING_LENGTH'], config['PERCENTAGE_MISSING_CLIENT'], config['MISSING_MODE'])\n",
    "if config['PREFIX'] == 'normal':\n",
    "    history_client_save_dir = os.path.join('model_history', config['DATASET'], config['PREFIX'], 'clients')\n",
    "    \n",
    "if not os.path.exists(history_client_save_dir):\n",
    "    os.makedirs(history_client_save_dir)\n",
    "\n",
    "FLDigitalTwin.to_json(f\"{history_client_save_dir}/losses_rmses.json\", history_client_normal_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get loss of central model after training all round with all clients\n",
    "central_x_train, central_y_train, central_x_test, central_y_test, _ = train_test_dataset[2]\n",
    "central_model = FLDigitalTwin.create_model(input_shape=(1, config['LOOK_BACK']), output_shape=1)\n",
    "weights = [model.get_weights() for model in model_arr]\n",
    "new_weights = [\n",
    "            np.mean([w[i] for w in weights], axis=0) for i in range(len(weights[0]))\n",
    "        ]\n",
    "central_model.set_weights(new_weights)\n",
    "central_history = central_model.fit(central_x_train, central_y_train, validation_data=(central_x_test, central_y_test), epochs=config['EPOCHS'], batch_size=config['BATCH_SIZE'], verbose=1)\n",
    "\n",
    "losses = central_history.history['loss']\n",
    "rmses = [x**(1/2) for x in central_history.history['mse']]\n",
    "maes = central_history.history['mae']\n",
    "val_losses = central_history.history['val_loss']\n",
    "val_rmses = [x**(1/2) for x in central_history.history['val_mse']]\n",
    "val_maes = central_history.history['val_mae']\n",
    "\n",
    "losses_rmses_dict = {\n",
    "    \"losses\": losses,\n",
    "    \"rmses\": rmses,\n",
    "    \"maes\": maes,\n",
    "    \"val_losses\": val_losses,\n",
    "    \"val_rmses\": val_rmses,\n",
    "    \"val_maes\": val_maes\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "central_predictions = central_model.predict(central_x_test)\n",
    "print(\"Central model prediction/test RMSE: {}\".format((mean_squared_error(central_y_test, central_predictions)**(1/2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_central_save_dir = os.path.join('model_history', config['DATASET'], config['PREFIX'], 'central', config['MATRIX_MISSING_LENGTH'], config['PERCENTAGE_MISSING_CLIENT'], config['MISSING_MODE'])\n",
    "if config['PREFIX'] == 'normal':\n",
    "    history_central_save_dir = os.path.join('model_history', config['DATASET'], config['PREFIX'], 'central')\n",
    "if not os.path.exists(history_central_save_dir):\n",
    "    os.makedirs(history_central_save_dir)\n",
    "\n",
    "FLDigitalTwin.to_json(f\"{history_central_save_dir}/losses_rmses.json\", losses_rmses_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, len(losses) + 1), losses, label=f'Central Model Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title(f'Training Loss for Central Model')\n",
    "plt.show()\n",
    "\n",
    "# RMSE\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, len(rmses) + 1), rmses, label=f'Central Model Training RMSE')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('RMSE')\n",
    "plt.legend()\n",
    "plt.title(f'Training Loss for Central Model')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predictions_arr = []\n",
    "for i in range(config['NUM_CLIENTS']):\n",
    "    _x_train, _y_train, _x_test, _y_test, _sc_cl = train_test_dataset[i]\n",
    "    _train_predictions = FLDigitalTwin.inverse_transform_predictions(model_arr[i].predict(_x_train), _sc_cl)\n",
    "    train_predictions_arr.append(_train_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions_arr = []\n",
    "for i in range(config['NUM_CLIENTS']):\n",
    "    _x_train, _y_train, _x_test, _y_test, _sc_cl = train_test_dataset[i]\n",
    "    _test_predictions = FLDigitalTwin.inverse_transform_predictions(model_arr[i].predict(_x_test), _sc_cl)\n",
    "    test_predictions_arr.append(_test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(config['NUM_CLIENTS']):\n",
    "    FLDigitalTwin.prepare_and_plot(\n",
    "        dictionary[f'client_{i}']['Daily Value'],\n",
    "        train_predictions_arr[i],\n",
    "        test_predictions_arr[i],\n",
    "        config['LOOK_BACK'],\n",
    "        f\"FL Model - Client {i}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the predictions\n",
    "output_dir = os.path.join(config['FL_OUTPUT_DIR'], config['PREFIX'] + config['MATRIX_MISSING_LENGTH'], config['PERCENTAGE_MISSING_CLIENT'], config['MISSING_MODE'])\n",
    "if config['PREFIX'] == 'normal':\n",
    "    output_dir = os.path.join(config['FL_OUTPUT_DIR'], config['PREFIX'])\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "predictions_files = {}\n",
    "for i in range(config['NUM_CLIENTS']):\n",
    "    predictions_files[f'client{i}_test_predictions'] = test_predictions_arr[i]\n",
    "    predictions_files[f'client{i}_train_predictions'] = train_predictions_arr[i]\n",
    "    predictions_files[f'client{i}_original'] = dictionary[f'client_{i}']['Daily Value']\n",
    "\n",
    "for filename, data in predictions_files.items():\n",
    "    pd.DataFrame(data).to_excel(os.path.join(output_dir, filename + '.xlsx'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
