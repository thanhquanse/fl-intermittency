{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "import tensorflow as tf\n",
    "import yaml\n",
    "import argparse\n",
    "\n",
    "from keras.callbacks import TensorBoard\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from libs.FLDigitalTwin import FLDigitalTwin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--dataset\", type=str, default=\"traffic\")\n",
    "parser.add_argument(\"--prefix\", type=str, default=\"normal\")\n",
    "parser.add_argument(\"--percent_mc\", type=str, default=\"01\")\n",
    "parser.add_argument(\"--missing_mode\", type=str, default=\"noadjacency\")\n",
    "parser.add_argument(\"--matrix_ml\", type=str, default=\"10x10\")\n",
    "parser.add_argument(\"--weight_mechanism\", type=int, default=0)\n",
    "parser.add_argument(\"--is_cluster\", type=str, default=\"no\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get values defined from CLI\n",
    "CONFIG_FILE = '.config_ipynb'\n",
    "if os.path.isfile(CONFIG_FILE):\n",
    "    with open(CONFIG_FILE) as f:\n",
    "        _args = f.read().split()\n",
    "        _args = _args[1:]\n",
    "else:\n",
    "    _args = ['--dataset', 'electricity', '--prefix', 'avg', '--percent_mc', '01', '--missing_mode', 'noadjacency', '--is_cluster no']\n",
    "\n",
    "args = parser.parse_args(_args)\n",
    "\n",
    "# Get configs from yaml\n",
    "with open('config.yaml', 'r') as file:\n",
    "    yaml_data = yaml.load(file, Loader=yaml.SafeLoader)\n",
    "\n",
    "config = {\n",
    "    'DATASET': args.dataset if args.dataset else yaml_data['DATASET'],\n",
    "    'NUM_DATA_SHEETS': yaml_data['NUM_DATA_SHEETS'],\n",
    "    'NUM_CLIENTS': yaml_data['NUM_CLIENTS'],\n",
    "    \"LOOK_BACK\": yaml_data['LOOK_BACK'],\n",
    "    'EPOCHS': yaml_data['EPOCHS'],\n",
    "    'CLIENT_EPOCHS': yaml_data['CLIENT_EPOCHS'],\n",
    "    'BATCH_SIZE': yaml_data['BATCH_SIZE'],\n",
    "    'SAVE_INTERVAL': yaml_data['SAVE_INTERVAL'],\n",
    "    'LEARNING_RATE': yaml_data['LEARNING_RATE'],\n",
    "    'TRAIN_SIZE': yaml_data['TRAIN_SIZE'],\n",
    "    'TRAIN_ROUNDS': yaml_data['TRAIN_ROUNDS'],\n",
    "    'DATA_DIR': yaml_data['DATA_DIR'],\n",
    "    'CLIENT_MATRIX_DIR': yaml_data['CLIENT_MATRIX_DIR'],\n",
    "    'FL_OUTPUT_DIR': yaml_data['FL_OUTPUT_DIR'],\n",
    "    'GENERAL_OUTPUT_DIR': yaml_data['GENERAL_OUTPUT_DIR'],\n",
    "    'PREFIX': args.prefix if args.prefix else yaml_data['PREFIX'],\n",
    "    'PERCENTAGE_MISSING_CLIENT': args.percent_mc if args.percent_mc else yaml_data['PERCENTAGE_MISSING_CLIENT'],\n",
    "    'MISSING_MODE': args.missing_mode if args.missing_mode else yaml_data['MISSING_MODE'],\n",
    "    'MATRIX_MISSING_LENGTH': args.matrix_ml if args.matrix_ml else yaml_data['MATRIX_MISSING_LENGTH'],\n",
    "    'IS_CLUSTER': args.is_cluster if args.is_cluster else yaml_data['IS_CLUSTER'],\n",
    "    'WEIGHT_MECHANISM': args.weight_mechanism if args.weight_mechanism else 0,\n",
    "    'WEIGHT_TRACKING_DIR': os.path.join('model_weight_track', args.dataset if args.dataset else yaml_data['DATASET'], args.prefix if args.prefix else yaml_data['PREFIX'], args.matrix_ml if args.matrix_ml else yaml_data['MATRIX_MISSING_LENGTH'], args.missing_mode if args.missing_mode else yaml_data['MISSING_MODE'])\n",
    "}\n",
    "\n",
    "print(config)\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Set tf random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(3)  # Set random seed for reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Initialize FL-DT Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FLDT = FLDigitalTwin(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5.1: Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the file path and sheet names with their corresponding keys\n",
    "file_path = f\"{config['DATA_DIR']}/{config['DATASET']}/standard_{config['DATASET']}.xlsx\"\n",
    "sheets = {}\n",
    "for i in range(config['NUM_DATA_SHEETS']):\n",
    "    sheets[f'client_{str(i)}'] = f'Sheet_{str(i + 1)}'\n",
    "\n",
    "# Load the data\n",
    "dictionary = FLDT.load_data(file_path, sheets)\n",
    "val_dictionary = FLDT.load_val_data(file_path, sheets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5.2: Example for client_1 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_name = \"client_1\"\n",
    "df = dictionary[client_name]\n",
    "df_val = val_dictionary[client_name]\n",
    "# df[\"value\"] = df.iloc[:, 1:-1].sum(axis=1)\n",
    "\n",
    "# Visualize data\n",
    "FLDT.visualize_data(df, \"Energy Distribution (kWh)\", \"Value (kWh)\", \"Frequency\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5.3: Split train/test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale and split data\n",
    "training_set, test_data, sc_X = FLDT.scale_split_datasets(\n",
    "    df[\"value\"], config['TRAIN_SIZE'], config['LOOK_BACK']\n",
    ")\n",
    "x_train, y_train = FLDT.create_rnn_dataset(training_set, config['LOOK_BACK'])\n",
    "x_test, y_test = FLDT.create_rnn_dataset(test_data, config['LOOK_BACK'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5.4: Create models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train the model\n",
    "ts_model = FLDT.create_model(input_shape=(1, config['LOOK_BACK']), output_shape=1)\n",
    "log_dir = f\"logs/{config['PREFIX']}fit/\" + dt.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "# with tf.device('/GPU:0'):\n",
    "ts_model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    epochs=config['CLIENT_EPOCHS'],\n",
    "    batch_size=config['BATCH_SIZE'],\n",
    "    verbose=1,\n",
    "    callbacks=[tensorboard_callback],\n",
    ")\n",
    "\n",
    "# Evaluate model\n",
    "ts_model.evaluate(x_test, y_test, verbose=1)\n",
    "predict_on_train = ts_model.predict(x_train)\n",
    "predict_on_test = ts_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5.5: Try plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_on_train = sc_X.inverse_transform(predict_on_train)\n",
    "predict_on_test = sc_X.inverse_transform(predict_on_test)\n",
    "\n",
    "# Plot predictions\n",
    "plot_original, plot_train, plot_test = FLDT.plot_data_preparation(\n",
    "    df[\"value\"], predict_on_train, predict_on_test, config['LOOK_BACK']\n",
    ")\n",
    "FLDT.plot_the_data(plot_original, plot_train, plot_test, \"Model Predictions vs Actual\")\n",
    "\n",
    "# Seasonal decomposition\n",
    "result = seasonal_decompose(df[\"value\"], model=\"additive\", period=365)\n",
    "result.plot()\n",
    "plt.suptitle(\"Seasonal Decomposition of Value\")\n",
    "plt.show()\n",
    "\n",
    "# Enhanced visualization of seasonal decomposition\n",
    "fig, axes = plt.subplots(4, 1, figsize=(15, 12), sharex=True)\n",
    "result.observed.plot(ax=axes[0], title=\"Observed\")\n",
    "result.trend.plot(ax=axes[1], title=\"Trend\")\n",
    "result.seasonal.plot(ax=axes[2], title=\"Seasonal\")\n",
    "result.resid.plot(ax=axes[3], title=\"Residual\")\n",
    "for ax in axes:\n",
    "    ax.set_ylabel(\"Value Summation\")\n",
    "axes[3].set_xlabel(\"Time\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5.6: Try saving to excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the residuals to an Excel file\n",
    "os.makedirs(os.path.join(config['GENERAL_OUTPUT_DIR'], client_name, config['PREFIX']), exist_ok=True)\n",
    "pd.DataFrame(result.resid).to_excel(\n",
    "    os.path.join(config['GENERAL_OUTPUT_DIR'], client_name, config['PREFIX'], '_client.xlsx'), sheet_name=\"Decomposition_Residuals\"\n",
    ")\n",
    "\n",
    "# Summary statistics\n",
    "residual_stats = pd.DataFrame(\n",
    "    {\n",
    "        \"Mean\": [result.resid.mean()],\n",
    "        \"Median\": [result.resid.median()],\n",
    "        \"Standard Deviation\": [result.resid.std()],\n",
    "        \"Max\": [result.resid.max()],\n",
    "        \"Min\": [result.resid.min()],\n",
    "    }\n",
    ")\n",
    "\n",
    "# Save summary statistics\n",
    "residual_stats.to_excel(os.path.join(config['GENERAL_OUTPUT_DIR'], client_name, config['PREFIX'], 'residual_statistics.xlsx'), index=False)\n",
    "\n",
    "# Display summary statistics\n",
    "print(\"Summary Statistics of Residuals:\")\n",
    "print(residual_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5.7: Evaluate models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation metrics\n",
    "metrics = ts_model.evaluate(x_test, y_test, verbose=1)\n",
    "metrics_df = pd.DataFrame([metrics], columns=[\"Loss\", \"MSE\", \"MAE\", \"MAPE\"])\n",
    "\n",
    "# Save evaluation metrics\n",
    "metrics_df.to_excel(os.path.join(config['GENERAL_OUTPUT_DIR'], client_name, config['PREFIX'], 'evaluation_metrics.xlsx'), index=False)\n",
    "\n",
    "# Display evaluation metrics\n",
    "print(\"Evaluation Metrics:\")\n",
    "print(metrics_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5.8: Create train/test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Central\n",
    "# central_df = pd.DataFrame(columns=['date', 'value'])\n",
    "# central_train_test_dataset = []\n",
    "# Client\n",
    "train_test_dataset = []\n",
    "val_dataset = []\n",
    "for i in range(config['NUM_DATA_SHEETS']):\n",
    "    _x_train, _y_train, _x_test, _y_test, _sc_cl = FLDT.create_train_test_dataset(dictionary[f'client_{str(i)}'], config['LOOK_BACK'])\n",
    "    train_test_dataset.append((_x_train, _y_train, _x_test, _y_test, _sc_cl))\n",
    "\n",
    "    _val_x_train, _val_y_train, _val_x_test, _val_y_test, _val_sc_cl = FLDT.create_train_test_dataset(val_dictionary[f'client_{str(i)}'], config['LOOK_BACK'])\n",
    "    val_dataset.append((_val_x_train, _val_y_train, _val_x_test, _val_y_test, _val_sc_cl))\n",
    "\n",
    "    # val_central_df = val_dictionary[f'client_{str(i)}']\n",
    "    # central_df = pd.concat([central_df, ], axis=0, ignore_index=True)\n",
    "\n",
    "# central_train_x, central_train_y, central_test_x, central_test_y, _central_sc_cl = FLDT.create_train_test_dataset(central_df[:2000], config['LOOK_BACK'])\n",
    "central_train_x, central_train_y, central_test_x, central_test_y, _central_sc_cl = FLDT.create_train_test_dataset(val_dictionary['client_0'], config['LOOK_BACK'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5.9: Create model arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_arr = []\n",
    "for i in range(config['NUM_CLIENTS']):\n",
    "    _model = FLDT.create_model(input_shape=(1, config['LOOK_BACK']), output_shape=1)\n",
    "    model_arr.append(_model)\n",
    "\n",
    "central_model = FLDT.create_model(input_shape=(1, config['LOOK_BACK']), output_shape=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5.10: Train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(config['NUM_CLIENTS']):\n",
    "#     _x_train, _y_train, _, _, _ = train_test_dataset[i]\n",
    "#     FLDT.train_model(model_arr[i], _x_train, _y_train, os.path.join('logs', 'fit', config['PREFIX'], str(i)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5.11: Evaluate models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(config['NUM_CLIENTS']):\n",
    "#     _x_train, _y_train, _x_test, _y_test, _ = train_test_dataset[i]\n",
    "#     FLDT.evaluate_model(model_arr[i], _x_train, _y_train, _x_test, _y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Execute the full client update scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_matrix_prefix = 'client_matrix_' + config['MATRIX_MISSING_LENGTH']\n",
    "if config['IS_CLUSTER'] != 'no':\n",
    "    client_matrix_prefix = config['IS_CLUSTER'] + '_' + client_matrix_prefix\n",
    "client_matrix_path = os.path.join(config['CLIENT_MATRIX_DIR'], client_matrix_prefix + '_' + config['PERCENTAGE_MISSING_CLIENT'] + '_' + config['MISSING_MODE'] + '.csv')\n",
    "client_matrix = np.loadtxt(client_matrix_path, delimiter=',', dtype=str)\n",
    "print(client_matrix)\n",
    "\n",
    "if not os.path.exists(config['WEIGHT_TRACKING_DIR']) and config['PREFIX'] == 'weight':\n",
    "    os.makedirs(config['WEIGHT_TRACKING_DIR'])\n",
    "\n",
    "history_client_normal_dict = {}\n",
    "init_weights = [model.get_weights() for model in model_arr]\n",
    "global_weights = [ np.mean([w[i] for w in init_weights], axis=0) for i in range(len(init_weights[0])) ]\n",
    "central_model_evaluation_rounds = []\n",
    "\n",
    "for r in range(config['TRAIN_ROUNDS']):\n",
    "    print(f'----------------------------------Round {r}-------------------------------------')\n",
    "    \n",
    "    fake_model = FLDT.create_model(input_shape=(1, config['LOOK_BACK']), output_shape=1)\n",
    "    \n",
    "    history_client_normal_dict[str(r)] = {}\n",
    "    weights_per_round = []\n",
    "\n",
    "    central_model.set_weights(global_weights)\n",
    "    _, mse, _, _ = central_model.evaluate(central_test_x, central_test_y, verbose=0)\n",
    "\n",
    "    rmse = mse**(1/2)\n",
    "    central_model_evaluation_rounds.append(rmse)\n",
    "\n",
    "    for i in range(len(model_arr)):\n",
    "        print(f\"Processing with dataset {i}...\")\n",
    "        _x_train, _y_train, _x_test, _y_test, _ = train_test_dataset[i]\n",
    "        if args.prefix == 'normal':\n",
    "            print(\"Run with FedNorm enabled...\")\n",
    "            # hist_dict_normal = FLDigitalTwin.train_fl_full_updates(model_arr, _x_train, _y_train, _x_test, _y_test)\n",
    "            hist_dict_normal = FLDT._train_fl_full_updates(model_arr, _x_train, _y_train, _x_test, _y_test, i, global_weights)\n",
    "            weights_per_round.append(model_arr[i].get_weights())\n",
    "            fake_model.set_weights(model_arr[i].get_weights())\n",
    "        else:\n",
    "            hist_dict_normal = FLDT._train_fl_full_updates(model_arr, _x_train, _y_train, _x_test, _y_test, i, global_weights)\n",
    "            if r >= 2 and client_matrix[r, i] == 'N':\n",
    "                print(f\"Client {i} has no update at round {r}...\")\n",
    "                if args.prefix == 'weight':\n",
    "                    print(f\"Run with FedDT enabled...\")\n",
    "\n",
    "                    csv_filename1 = f'model_client_{str(i)}_round_{str(r - 2)}.csv'\n",
    "                    csv_filename2 = f'model_client_{str(i)}_round_{str(r - 1)}.csv'\n",
    "\n",
    "                    prev_csv_1 = os.path.join(config['WEIGHT_TRACKING_DIR'], csv_filename1)\n",
    "                    prev_csv_2 = os.path.join(config['WEIGHT_TRACKING_DIR'], csv_filename2)\n",
    "                    \n",
    "                    print(f\"Calculating weights using the sum of round {r - 1} and {r - 2}...\")\n",
    "                    weight_dt = FLDT.sum_weights_from_two_csvs(prev_csv_1, prev_csv_2, model_arr[i])\n",
    "\n",
    "                    weights_per_round.append(weight_dt)\n",
    "                    fake_model.set_weights(weight_dt)  \n",
    "                else:\n",
    "                    print(f\"Run with FedAvg enabled...\")\n",
    "                    zero_w = [0.0*np.random.rand(*w.shape) for w in model_arr[i].get_weights()]\n",
    "                    \n",
    "                    weights_per_round.append(zero_w)\n",
    "                    fake_model.set_weights(zero_w)\n",
    "            else:\n",
    "                weights_per_round.append(model_arr[i].get_weights())\n",
    "                fake_model.set_weights(model_arr[i].get_weights())\n",
    "\n",
    "            if args.prefix == 'weight':\n",
    "                csv_filename = f'model_client_{str(i)}_round_{str(r)}.csv'\n",
    "                csv_file = os.path.join(config['WEIGHT_TRACKING_DIR'], csv_filename)\n",
    "                FLDT.save_weights_to_csv(fake_model, csv_file)\n",
    "                print(f\"Saved weights to {csv_file}\")\n",
    "\n",
    "        history_client_normal_dict[str(r)][f'client_{str(i)}'] = hist_dict_normal\n",
    "\n",
    "    weights = [w for w in weights_per_round]\n",
    "    global_weights = [\n",
    "        np.mean([w[i] for w in weights], axis=0) for i in range(len(weights[0]))\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_prefix = 'model_history'\n",
    "if args.is_cluster != 'no':\n",
    "    outputs_prefix = \"model_history/\" + args.is_cluster\n",
    "history_client_save_dir = os.path.join(outputs_prefix, config['DATASET'], config['PREFIX'], 'clients', config['MATRIX_MISSING_LENGTH'], config['PERCENTAGE_MISSING_CLIENT'], config['MISSING_MODE'])\n",
    "if config['PREFIX'] == 'normal':\n",
    "    history_client_save_dir = os.path.join(outputs_prefix, config['DATASET'], config['PREFIX'], 'clients')\n",
    "    \n",
    "if not os.path.exists(history_client_save_dir):\n",
    "    os.makedirs(history_client_save_dir)\n",
    "\n",
    "FLDT.to_json(f\"{history_client_save_dir}/losses_rmses.json\", history_client_normal_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get loss of central model after training all round with all clients\n",
    "# central_x_train, central_y_train, central_x_test, central_y_test, _ = train_test_dataset[0]\n",
    "# central_model = FLDT.create_model(input_shape=(1, config['LOOK_BACK']), output_shape=1)\n",
    "# # weights = [model.get_weights() for model in model_arr]\n",
    "# # new_weights = [\n",
    "# #             np.mean([w[i] for w in weights], axis=0) for i in range(len(weights[0]))\n",
    "# #         ]\n",
    "# central_model.set_weights(global_weights)\n",
    "# rmse_arr = []\n",
    "# for val_data in train_test_dataset:\n",
    "#     _, _, val_x_test, val_y_test, _ = val_data\n",
    "#     predictions = central_model.predict(val_x_test)\n",
    "#     rmse_arr.append((mean_squared_error(val_y_test, predictions)**(1/2)))\n",
    "\n",
    "# losses_rmses_dict = {\n",
    "#     \"rmses\": rmse_arr\n",
    "# }\n",
    "\n",
    "# print(f\"Central model cross prediction/test RMSE: {np.mean(rmse_arr)}\")\n",
    "# central_history = central_model.fit(central_x_train, central_y_train, validation_data=(central_x_test, central_y_test), epochs=config['EPOCHS'], batch_size=config['BATCH_SIZE'], verbose=1)\n",
    "\n",
    "# losses = central_history.history['loss']\n",
    "# rmses = [x**(1/2) for x in central_history.history['mse']]\n",
    "# maes = central_history.history['mae']\n",
    "# val_losses = central_history.history['val_loss']\n",
    "# val_rmses = [x**(1/2) for x in central_history.history['val_mse']]\n",
    "# val_maes = central_history.history['val_mae']\n",
    "\n",
    "central_model.set_weights(global_weights)\n",
    "rmses = []\n",
    "losses = []\n",
    "maes = []\n",
    "for val_data in val_dataset:\n",
    "    _, _, val_x_test, val_y_test, _ = val_data\n",
    "    loss, mse, mae, mape = central_model.evaluate(val_x_test, val_y_test, verbose=1)\n",
    "    rmses.append(mse**(1/2))\n",
    "    losses.append(loss)\n",
    "    maes.append(mae)\n",
    "\n",
    "central_loss, central_mse, central_mae, central_mape = central_model.evaluate(central_test_x, central_test_y, verbose=1)\n",
    "\n",
    "\n",
    "losses_rmses_dict = {\n",
    "    \"central_round_predictions\": central_model_evaluation_rounds,\n",
    "    \"client_predictions\": {\n",
    "        \"losses\": losses,\n",
    "        \"rmses\": rmses,\n",
    "        \"maes\": maes\n",
    "    },\n",
    "    \"central_predictions\": {\n",
    "        \"loss\": central_loss,\n",
    "        \"rmse\": central_mse**(1/2),\n",
    "        \"mae\": central_mae\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(\n",
    "    monitor=\"val_loss\", min_delta=0.001, patience=5, verbose=1, mode=\"auto\", restore_best_weights=True\n",
    ")\n",
    "central_history = central_model.fit(\n",
    "    central_train_x, \n",
    "    central_train_y, \n",
    "    validation_data=(central_test_x, central_test_y), \n",
    "    epochs=config['EPOCHS'], \n",
    "    batch_size=config['BATCH_SIZE'], \n",
    "    verbose=1,\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "refit_losses = central_history.history['loss']\n",
    "refit_rmses = [x**(1/2) for x in central_history.history['mse']]\n",
    "refit_maes = central_history.history['mae']\n",
    "refit_val_losses = central_history.history['val_loss']\n",
    "refit_val_rmses = [x**(1/2) for x in central_history.history['val_mse']]\n",
    "refit_val_maes = central_history.history['val_mae']\n",
    "\n",
    "losses_rmses_dict[\"refit\"] = {\n",
    "    \"losses\": refit_losses,\n",
    "    \"rmses\": refit_rmses,\n",
    "    \"maes\": refit_maes,\n",
    "    \"val_losses\": refit_val_losses,\n",
    "    \"val_rmses\": refit_val_rmses,\n",
    "    \"val_maes\": refit_val_maes\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_central_save_dir = os.path.join(outputs_prefix, config['DATASET'], config['PREFIX'], 'central', config['MATRIX_MISSING_LENGTH'], config['PERCENTAGE_MISSING_CLIENT'], config['MISSING_MODE'])\n",
    "if config['PREFIX'] == 'normal':\n",
    "    history_central_save_dir = os.path.join(outputs_prefix, config['DATASET'], config['PREFIX'], 'central')\n",
    "if not os.path.exists(history_central_save_dir):\n",
    "    os.makedirs(history_central_save_dir)\n",
    "\n",
    "FLDT.to_json(f\"{history_central_save_dir}/losses_rmses.json\", losses_rmses_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, len(refit_losses) + 1), refit_losses, label=f'Train')\n",
    "plt.plot(range(1, len(refit_val_losses) + 1), refit_val_losses, label=f'Test')\n",
    "# plt.plot(losses, label='Train')\n",
    "# plt.plot(val_losses, label='Test')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title(f'Central Model Loss Refit')\n",
    "plt.show()\n",
    "\n",
    "# RMSE\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, len(refit_rmses) + 1), refit_rmses, label=f'Train')\n",
    "plt.plot(range(1, len(refit_val_rmses) + 1), refit_val_rmses, label=f'Test')\n",
    "# plt.plot(rmses, label=f'Train')\n",
    "# plt.plot(val_rmses, label=f'Test')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('RMSE')\n",
    "plt.legend()\n",
    "plt.title(f'Central Model RMSE Refit')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predictions_arr = []\n",
    "for i in range(config['NUM_CLIENTS']):\n",
    "    _x_train, _y_train, _x_test, _y_test, _sc_cl = train_test_dataset[i]\n",
    "    _train_predictions = FLDT.inverse_transform_predictions(model_arr[i].predict(_x_train), _sc_cl)\n",
    "    train_predictions_arr.append(_train_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions_arr = []\n",
    "for i in range(config['NUM_CLIENTS']):\n",
    "    _x_train, _y_train, _x_test, _y_test, _sc_cl = train_test_dataset[i]\n",
    "    _test_predictions = FLDT.inverse_transform_predictions(model_arr[i].predict(_x_test), _sc_cl)\n",
    "    test_predictions_arr.append(_test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(config['NUM_CLIENTS']):\n",
    "    FLDT.prepare_and_plot(\n",
    "        dictionary[f'client_{i}']['value'],\n",
    "        train_predictions_arr[i],\n",
    "        test_predictions_arr[i],\n",
    "        config['LOOK_BACK'],\n",
    "        f\"FL Model - Client {i}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the predictions\n",
    "output_dir = os.path.join(config['FL_OUTPUT_DIR'], config['PREFIX'] + config['MATRIX_MISSING_LENGTH'], config['PERCENTAGE_MISSING_CLIENT'], config['MISSING_MODE'])\n",
    "if config['PREFIX'] == 'normal':\n",
    "    output_dir = os.path.join(config['FL_OUTPUT_DIR'], config['PREFIX'])\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "predictions_files = {}\n",
    "for i in range(config['NUM_CLIENTS']):\n",
    "    predictions_files[f'client{i}_test_predictions'] = test_predictions_arr[i]\n",
    "    predictions_files[f'client{i}_train_predictions'] = train_predictions_arr[i]\n",
    "    predictions_files[f'client{i}_original'] = dictionary[f'client_{i}']['value']\n",
    "\n",
    "for filename, data in predictions_files.items():\n",
    "    pd.DataFrame(data).to_excel(os.path.join(output_dir, filename + '.xlsx'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
