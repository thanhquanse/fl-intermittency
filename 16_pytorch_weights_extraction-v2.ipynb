{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5be4af2c-c7a4-4ba9-8c25-c48900c54d6a",
   "metadata": {},
   "source": [
    "## Step 1: Import Libraries\n",
    "We begin by importing the necessary libraries: PyTorch for building and training the neural network, and NumPy for handling the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44b4385-5367-49ab-9105-051e9916d2fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# from time import sleep\n",
    "\n",
    "# from sklearn.linear_model import LinearRegression, SGDRegressor\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.base import BaseEstimator\n",
    "# from sklearn.svm import SVR\n",
    "# from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# # from tensorflow.python.keras.layers import Dense\n",
    "# # from tensorflow.python.keras.models import Sequential\n",
    "# # import tensorflow as tf\n",
    "# # from keras.api.models import Sequential\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense\n",
    "# from scikeras.wrappers import KerasRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4684b6",
   "metadata": {},
   "source": [
    "## Step 2: Define Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c9c8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of models in the federated learning setup\n",
    "NUM_MODELS = 5\n",
    "# Number of previous time points used for forecasting the next point\n",
    "PREVIOUS_LAG = 5\n",
    "# Number of epochs for training\n",
    "EPOCHS = 50\n",
    "# Interval at which to save weights\n",
    "SAVE_INTERVAL = 1\n",
    "# Initial learning rate\n",
    "LEARNING_RATE = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10dea02c",
   "metadata": {},
   "source": [
    "## Step 3: Create Dataset Directories and CSV Files\n",
    "We create separate directories for each model, and each directory contains a unique univariate time series dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76031c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directories for each model dataset\n",
    "for i in range(NUM_MODELS):\n",
    "    directory = f'model_{i}_data'\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "# Create example univariate time series data and save to CSV files\n",
    "np.random.seed(0)\n",
    "for i in range(NUM_MODELS):\n",
    "    data = np.sin(np.linspace(0, 50, 150) + np.random.normal(0, 0.5, 150))  # Generate sine wave with noise\n",
    "    df = pd.DataFrame(data, columns=['value'])\n",
    "    df.to_csv(f'model_{i}_data/time_series.csv', index=False)\n",
    "\n",
    "# Create dedicated central test dataset\n",
    "central_test_data = np.sin(np.linspace(0, 50, 150) + np.random.normal(0, 0.5, 150))  # Generate sine wave with noise\n",
    "central_test_df = pd.DataFrame(central_test_data, columns=['value'])\n",
    "central_test_df.to_csv('central_test_data.csv', index=False)\n",
    "\n",
    "# df\n",
    "# central_test_data\n",
    "# central_test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7bf3750",
   "metadata": {},
   "source": [
    "## Step 4: Load Data into Training and Test Sets\n",
    "We load the univariate time series data and prepare the training and test sets for each model. We use the defined lag to create input-output pairs for auto-regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c002d2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create input-output pairs for auto-regression\n",
    "def create_lagged_features(data, lag):\n",
    "    x, y = [], []\n",
    "    for i in range(len(data) - lag):\n",
    "        x.append(data[i:i + lag])\n",
    "        y.append(data[i + lag])\n",
    "    return np.array(x), np.array(y)\n",
    "\n",
    "# Prepare training and test sets for each model\n",
    "datasets = []\n",
    "for i in range(NUM_MODELS):\n",
    "    # Load time series data\n",
    "    df = pd.read_csv(f'model_{i}_data/time_series.csv')\n",
    "    data = df['value'].values\n",
    "\n",
    "    # Create lagged features\n",
    "    x, y = create_lagged_features(data, PREVIOUS_LAG)\n",
    "\n",
    "    # Split into training (80%) and test (20%) sets\n",
    "    train_size = int(0.8 * len(x))\n",
    "    x_train, x_test = x[:train_size], x[train_size:]\n",
    "    y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "    # Save datasets\n",
    "    np.save(f'model_{i}_data/x_train.npy', x_train)\n",
    "    np.save(f'model_{i}_data/y_train.npy', y_train)\n",
    "    np.save(f'model_{i}_data/x_test.npy', x_test)\n",
    "    np.save(f'model_{i}_data/y_test.npy', y_test)\n",
    "\n",
    "    datasets.append((x_train, y_train, x_test, y_test))\n",
    "\n",
    "# Prepare central test set\n",
    "central_test_df = pd.read_csv('central_test_data.csv')\n",
    "central_test_data = central_test_df['value'].values\n",
    "x_central_test, y_central_test = create_lagged_features(central_test_data, PREVIOUS_LAG)\n",
    "np.save('x_central_test.npy', x_central_test)\n",
    "np.save('y_central_test.npy', y_central_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0409ffda",
   "metadata": {},
   "source": [
    "## Step 5: Define the Neural Network\n",
    "We define a neural network with two hidden layers. The input size is determined by the `PREVIOUS_LAG` hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54dfe8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network\n",
    "class ComplexNet(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(ComplexNet, self).__init__()\n",
    "        self.hidden1 = nn.Linear(input_size, 10)\n",
    "        self.bn1 = nn.BatchNorm1d(10)\n",
    "        self.hidden2 = nn.Linear(10, 5)\n",
    "        self.bn2 = nn.BatchNorm1d(5)\n",
    "        self.output = nn.Linear(5, 1)\n",
    "        \n",
    "        # Initialize weights\n",
    "        nn.init.kaiming_uniform_(self.hidden1.weight, nonlinearity='relu')\n",
    "        nn.init.kaiming_uniform_(self.hidden2.weight, nonlinearity='relu')\n",
    "        nn.init.xavier_uniform_(self.output.weight)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.bn1(self.hidden1(x)))\n",
    "        x = torch.relu(self.bn2(self.hidden2(x)))\n",
    "        x = self.output(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1586927e",
   "metadata": {},
   "source": [
    "## Step 6: Train Multiple Models Independently and Save Weights\n",
    "We instantiate and train `NUM_MODELS` separate models using the corresponding datasets. During training, we save the weights of each model at every training round to a separate CSV file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22267ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train a single model and save weights\n",
    "def train_model(model_id):\n",
    "    # Load dataset for the model\n",
    "    directory = f'model_{model_id}_data'\n",
    "    x_train = np.load(os.path.join(directory, 'x_train.npy'))\n",
    "    y_train = np.load(os.path.join(directory, 'y_train.npy'))\n",
    "    x_train_tensor = torch.tensor(x_train, dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "    model = ComplexNet(input_size=PREVIOUS_LAG)\n",
    "    criterion = lambda output, target: torch.sqrt(nn.MSELoss()(output, target))\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "    csv_filename = f'weights_tracking_model_{model_id}.csv'\n",
    "    training_losses = []\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        # Forward pass\n",
    "        outputs = model(x_train_tensor)\n",
    "        loss = criterion(outputs, y_train_tensor)\n",
    "        training_losses.append(loss.item())\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        # Save weights at intervals\n",
    "        if epoch % SAVE_INTERVAL == 0:\n",
    "            # Extract weights and flatten them\n",
    "            weights_list = []\n",
    "            for name, param in model.named_parameters():\n",
    "                if param.requires_grad:\n",
    "                    weights_list.extend(param.data.view(-1).tolist())\n",
    "\n",
    "            # Check if CSV file exists\n",
    "            file_exists = os.path.isfile(csv_filename)\n",
    "\n",
    "            # Open CSV file to save weights\n",
    "            with open(csv_filename, mode='a', newline='') as file:\n",
    "                writer = csv.writer(file)\n",
    "\n",
    "                # If file doesn't exist, write the header\n",
    "                if not file_exists:\n",
    "                    header = []\n",
    "                    for name, param in model.named_parameters():\n",
    "                        if param.requires_grad:\n",
    "                            num_weights = param.data.numel()\n",
    "                            header.extend([f'{name}_weight_{i}' for i in range(num_weights)])\n",
    "                    writer.writerow(header)\n",
    "\n",
    "                # Write weights to CSV\n",
    "                writer.writerow(weights_list)\n",
    "\n",
    "    # Plot training loss\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(range(1, EPOCHS + 1), training_losses, label=f'Model {model_id} Training Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.title(f'Training Loss for Model {model_id}')\n",
    "    plt.show()\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# Train NUM_MODELS models independently and save their weights\n",
    "models = [train_model(i) for i in range(NUM_MODELS)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293b7df8",
   "metadata": {},
   "source": [
    "## Step 7: Load Weights from CSV Files and Compute the Average Weights\n",
    "We load the weights from the CSV files of the trained models, compute their average, and assign the averaged weights to a central model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "48adc553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 5)\n",
      "(10,)\n",
      "(10,)\n",
      "(10,)\n",
      "(5, 10)\n",
      "(5,)\n",
      "(5,)\n",
      "(5,)\n",
      "(1, 5)\n",
      "(1,)\n"
     ]
    }
   ],
   "source": [
    "central_model = ComplexNet(input_size=PREVIOUS_LAG)\n",
    "\n",
    "# Load weights from CSV files and compute the average weightsP\n",
    "with torch.no_grad():\n",
    "    for name, param in central_model.named_parameters():\n",
    "        weights = []\n",
    "        param_shape = param.data.size()\n",
    "        for i in range(NUM_MODELS):\n",
    "            csv_filename = f'weights_tracking_model_{i}.csv'\n",
    "            if not os.path.isfile(csv_filename):\n",
    "                continue  # Skip if the CSV file does not exist\n",
    "            with open(csv_filename, mode='r') as file:\n",
    "                reader = csv.reader(file)\n",
    "                next(reader)  # Skip header\n",
    "                last_row = None\n",
    "                for row in reader:\n",
    "                    last_row = row\n",
    "                if last_row:\n",
    "                    # Ensure the number of elements matches the parameter size\n",
    "                    if len(last_row) == param.numel():\n",
    "                        weights.append(torch.tensor([float(w) for w in last_row], dtype=torch.float32).view(param_shape))\n",
    "        if weights:\n",
    "            # Compute the average weight\n",
    "            avg_weight = torch.stack(weights).mean(dim=0)\n",
    "            param.copy_(avg_weight)\n",
    "\n",
    "        print(param.numpy().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f4e2d5",
   "metadata": {},
   "source": [
    "## Step 8: Use the Central Model to Make Predictions on the Central Test Set\n",
    "Finally, we use the central model to make predictions on the central test set and compute the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6191972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load central test set\n",
    "x_central_test = np.load('x_central_test.npy')\n",
    "y_central_test = np.load('y_central_test.npy')\n",
    "x_central_test_tensor = torch.tensor(x_central_test, dtype=torch.float32)\n",
    "y_central_test_tensor = torch.tensor(y_central_test, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "# Make predictions on the central test set and compute loss\n",
    "central_model.eval()\n",
    "criterion = lambda output, target: torch.sqrt(nn.MSELoss()(output, target))\n",
    "with torch.no_grad():\n",
    "    test_predictions = central_model(x_central_test_tensor)\n",
    "    test_loss = criterion(test_predictions, y_central_test_tensor)\n",
    "    print(\"Test Loss on Central Test Set:\", test_loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4baf79fb",
   "metadata": {},
   "source": [
    "## Step 9: Plot Weight Columns from a Weight Tracking CSV File\n",
    "We create a function to plot only the nodes in the last layer of the network to visualize how these specific weights evolve over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a142a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot weights in the last layer of the network from a weight tracking CSV file\n",
    "def plot_last_layer_weights(csv_filename):\n",
    "    if not os.path.isfile(csv_filename):\n",
    "        print(f\"File {csv_filename} does not exist.\")\n",
    "        return\n",
    "\n",
    "    # Load the CSV file into a DataFrame\n",
    "    df = pd.read_csv(csv_filename)\n",
    "\n",
    "    # Filter columns corresponding to the output layer weights\n",
    "    output_layer_columns = [col for col in df.columns if col.startswith('output')]\n",
    "    df_output_weights = df[output_layer_columns]\n",
    "\n",
    "    # Plot each weight in the output layer\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    for column in df_output_weights.columns:\n",
    "        plt.plot(df_output_weights[column], label=column)\n",
    "\n",
    "    plt.xlabel('Training Interval')\n",
    "    plt.ylabel('Weight Value')\n",
    "    plt.title(f'Weight Evolution for Output Layer in {csv_filename}')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Plot weights for the last layer of a specific model\n",
    "plot_last_layer_weights('weights_tracking_model_0.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396cbe0f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
