{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import Libraries\n",
    "We begin by importing the necessary libraries: PyTorch for building and training the neural network, and NumPy for handling the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-17 23:33:51.842519: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-17 23:33:53.122727: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-11-17 23:33:57.108043: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-11-17 23:33:57.108072: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-11-17 23:33:57.119588: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-17 23:33:58.832543: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-17 23:34:21.174608: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import tensorflow_federated as tff\n",
    "import collections\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2.1: Define Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of models in the federated learning setup\n",
    "NUM_CLIENTS = 100\n",
    "# Number of previous time points used for forecasting the next point\n",
    "SEQUENCE_LENGTH = 20\n",
    "# Number of epochs for training\n",
    "EPOCHS = 100\n",
    "# Interval at which to save weights\n",
    "SAVE_INTERVAL = 1\n",
    "# Initial learning rate\n",
    "LEARNING_RATE = 0.01\n",
    "# Client matrix dir\n",
    "CLIENT_MATRIX_DIR = \"client_status_data\"\n",
    "# Client matrix file\n",
    "CLIENT_MATRIX_FILE = \"client_status_random_on_off.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2.2: Create client status matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_csv_client_matrix(filename, rows=160, columns=100, percentage_Y=0.1):\n",
    "    # Initialize the previous row as empty to enforce the no-adjacency rule\n",
    "    prev_row = [''] * columns\n",
    "    \n",
    "    with open(filename, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        \n",
    "        for row_idx in range(rows):\n",
    "            row = ['Y'] * columns\n",
    "\n",
    "            if row_idx >= 3:\n",
    "                N_count = int(columns * percentage_Y)\n",
    "                \n",
    "                # Ensure that 'N' is placed in positions not occupied by 'N' in the previous row\n",
    "                possible_positions = [i for i in range(columns) if prev_row[i] != 'N']\n",
    "                N_positions = random.sample(possible_positions, min(N_count, len(possible_positions)))\n",
    "                \n",
    "                for pos in N_positions:\n",
    "                    row[pos] = 'N'\n",
    "            \n",
    "            # Write the current row to the CSV file\n",
    "            writer.writerow(row)\n",
    "            \n",
    "            # Update prev_row for the next iteration\n",
    "            prev_row = row\n",
    "\n",
    "create_csv_client_matrix(f'{CLIENT_MATRIX_DIR}/{CLIENT_MATRIX_FILE}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2.3: Extract time series subset for each client based on columns of the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_csv_subset(input_filename, output_filename, max_rows=1000, max_columns=100):\n",
    "    with open(input_filename, mode='r', newline='') as infile:\n",
    "        reader = csv.reader(infile)\n",
    "        with open(output_filename, mode='w', newline='') as outfile:\n",
    "            writer = csv.writer(outfile)\n",
    "            \n",
    "            for i, row in enumerate(reader):\n",
    "                if i >= max_rows:  # Stop after reaching the specified row limit\n",
    "                    break\n",
    "                # Write only the specified number of columns for each row\n",
    "                writer.writerow(row[:max_columns])\n",
    "\n",
    "# Extract train dataset\n",
    "extract_csv_subset(\"data/electricity.csv\", \"data/electricity_26000x100.csv\", max_rows=26000, max_columns=101)\n",
    "\n",
    "# Extract test dataset\n",
    "# extract_csv_subset(\"data/hourly_test.csv\", \"data/hourly_test_415x48.csv\", max_rows = 415, max_columns = 49)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create Dataset Directories and CSV Files\n",
    "We create separate directories for each model, and each directory contains a unique univariate time series dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.42335766 0.09489051 0.05839416 ... 0.17518248 0.08029197 0.45255474]\n",
      "[0.44401544 0.50579151 0.42471042 ... 0.38223938 0.3976834  0.5019305 ]\n",
      "[0.01331115 0.0249584  0.03327787 ... 0.01996672 0.01331115 0.01497504]\n",
      "[0.26495726 0.31623932 0.57008547 ... 0.33846154 0.44188034 0.44444444]\n",
      "[0.3071298  0.30347349 0.488117   ... 0.43875686 0.36745887 0.28884826]\n",
      "[0.41985887 0.42741935 0.56955645 ... 0.40322581 0.38356855 0.33064516]\n",
      "[0.07386364 0.07386364 0.09090909 ... 0.09090909 0.10227273 0.06818182]\n",
      "[0.48267202 0.48116524 0.63636364 ... 0.5183325  0.49171271 0.51682572]\n",
      "[0.46       0.72333333 0.52166667 ... 0.58833333 0.40833333 0.39166667]\n",
      "[0.50862069 0.33477011 0.49712644 ... 0.22988506 0.21695402 0.16091954]\n",
      "[0.37142857 0.37402597 0.54285714 ... 0.49090909 0.36103896 0.37142857]\n",
      "[0.80142687 0.50891795 0.20927467 ... 0.56718193 0.1783591  0.38049941]\n",
      "[0.56360078 0.49510763 0.46379648 ... 0.78277886 0.24657534 0.32876712]\n",
      "[0.30446927 0.33240223 0.53631285 ... 0.31005587 0.43575419 0.33798883]\n",
      "[0.52997602 0.46043165 0.71223022 ... 0.47242206 0.44844125 0.53956835]\n",
      "[0.45039766 0.5198828  0.42235245 ... 0.47802428 0.31142738 0.36207618]\n",
      "[0.36470588 0.43529412 0.52941176 ... 0.44705882 0.45882353 0.43529412]\n",
      "[0.37817259 0.46954315 0.64720812 ... 0.33756345 0.34010152 0.43401015]\n",
      "[0.30065359 0.32780292 0.4379085  ... 0.40522876 0.28557064 0.34188034]\n",
      "[0.71710526 0.43092105 0.33881579 ... 0.51973684 0.30592105 0.46052632]\n",
      "[0.19047619 0.34126984 0.32539683 ... 0.38888889 0.31746032 0.31746032]\n",
      "[0.32488889 0.53688889 0.68       ... 0.39333333 0.33688889 0.39333333]\n",
      "[0.24302789 0.29083665 0.57768924 ... 0.39840637 0.3187251  0.34262948]\n",
      "[0.33920705 0.2907489  0.59471366 ... 0.29515419 0.36563877 0.29515419]\n",
      "[0.3030303  0.34632035 0.61760462 ... 0.43722944 0.35642136 0.37662338]\n",
      "[0.69475358 0.71065183 0.3354531  ... 0.63116057 0.25755167 0.51828299]\n",
      "[0.3625     0.41477273 0.64318182 ... 0.43181818 0.32954545 0.46022727]\n",
      "[0.20560748 0.1635514  0.30373832 ... 0.17757009 0.30841121 0.25233645]\n",
      "[0.33581395 0.36837209 0.56651163 ... 0.49488372 0.40465116 0.40930233]\n",
      "[0.64339623 0.2245283  0.10188679 ... 0.62075472 0.08679245 0.16981132]\n",
      "[0.30843585 0.4086116  0.46485062 ... 0.49824253 0.33040422 0.30667838]\n",
      "[0.65879828 0.47961373 0.65665236 ... 0.62982833 0.4388412  0.56652361]\n",
      "[0.3250189  0.53665911 0.68027211 ... 0.39380197 0.33711262 0.09448224]\n",
      "[0.4527845  0.33898305 0.47215496 ... 0.48668281 0.32929782 0.44067797]\n",
      "[0.3777995  0.33736225 0.65863846 ... 0.38188766 0.3556701  0.45263064]\n",
      "[0.30573248 0.31528662 0.52547771 ... 0.3566879  0.43312102 0.41082803]\n",
      "[0.65845824 0.67344754 0.248394   ... 0.69271949 0.12312634 0.34689507]\n",
      "[0.4280303  0.3530303  0.42424242 ... 0.46060606 0.40378788 0.39621212]\n",
      "[0.54275093 0.57620818 0.48698885 ... 0.55390335 0.47211896 0.42007435]\n",
      "[0.33125 0.34375 0.55    ... 0.46875 0.3125  0.41875]\n",
      "[0.41712863 0.42934721 0.63480032 ... 0.4933816  0.38884489 0.4490327 ]\n",
      "[0.46026895 0.45171149 0.63202934 ... 0.599022   0.30134474 0.44437653]\n",
      "[0.38229755 0.43126177 0.36346516 ... 0.3653484  0.42184557 0.43502825]\n",
      "[0.32238806 0.54029851 0.57014925 ... 0.48358209 0.37313433 0.4       ]\n",
      "[0.29935484 0.38709677 0.57935484 ... 0.30193548 0.32903226 0.36516129]\n",
      "[0.52743363 0.49380531 0.60530973 ... 0.58053097 0.42831858 0.53097345]\n",
      "[0.17585302 0.15748031 0.38057743 ... 0.42257218 0.19947507 0.17060367]\n",
      "[0.3286119  0.45609065 0.52124646 ... 0.40793201 0.49291785 0.41926346]\n",
      "[0.27700831 0.23545706 0.34903047 ... 0.30470914 0.29639889 0.28808864]\n",
      "[0.31007752 0.33887043 0.42857143 ... 0.35991141 0.39424142 0.38648948]\n",
      "[0.54158004 0.51039501 0.66632017 ... 0.6018711  0.34511435 0.31704782]\n",
      "[0.40677966 0.52966102 0.68644068 ... 0.55932203 0.55084746 0.37288136]\n",
      "[0.4850214  0.38373752 0.50927247 ... 0.64336662 0.38659058 0.29671897]\n",
      "[0.36020583 0.40480274 0.56946827 ... 0.36535163 0.35334477 0.38250429]\n",
      "[0.18613139 0.21532847 0.4379562  ... 0.22992701 0.40145985 0.29562044]\n",
      "[0.50519931 0.42839137 0.50110288 ... 0.54955097 0.36576335 0.49708524]\n",
      "[0.34133791 0.40823328 0.58147513 ... 0.30360206 0.45454545 0.44082333]\n",
      "[0.00189036 0.43478261 0.54253308 ... 0.42722117 0.00378072 0.2778828 ]\n",
      "[0.35332334 0.45977011 0.46476762 ... 0.3878061  0.37381309 0.35382309]\n",
      "[0.16705336 0.13225058 0.25475638 ... 0.66032483 0.25800464 0.21299304]\n",
      "[0.50693481 0.4445215  0.5332871  ... 0.57004161 0.42926491 0.46324549]\n",
      "[0.37931034 0.39901478 0.60344828 ... 0.48029557 0.39408867 0.36206897]\n",
      "[0.40277778 0.40046296 0.61805556 ... 0.48148148 0.37037037 0.4212963 ]\n",
      "[0.3844367  0.54703833 0.70150987 ... 0.50058072 0.41231127 0.3437863 ]\n",
      "[0.4097561  0.42601626 0.27642276 ... 0.18211382 0.27479675 0.4       ]\n",
      "[0.37468354 0.42278481 0.65189873 ... 0.40379747 0.43670886 0.4278481 ]\n",
      "[0.33471933 0.4012474  0.52598753 ... 0.33887734 0.38461538 0.42411642]\n",
      "[0.33441558 0.37824675 0.49188312 ... 0.30194805 0.42694805 0.33603896]\n",
      "[0.59796438 0.32569975 0.28498728 ... 0.7913486  0.23918575 0.35877863]\n",
      "[0.32873563 0.44137931 0.49655172 ... 0.58850575 0.31724138 0.34942529]\n",
      "[0.2811245  0.1746988  0.21285141 ... 0.16064257 0.19477912 0.28714859]\n",
      "[0.21604938 0.45679012 0.36419753 ... 0.40740741 0.33950617 0.42592593]\n",
      "[0.36068896 0.5035461  0.43465046 ... 0.51671733 0.33637285 0.32421479]\n",
      "[0.54133333 0.352      0.60266667 ... 0.59733333 0.46666667 0.51466667]\n",
      "[0.37254902 0.41316527 0.58963585 ... 0.4047619  0.41176471 0.33893557]\n",
      "[0.49638452 0.50574224 0.3258188  ... 0.70778392 0.26797108 0.33517652]\n",
      "[0.31059246 0.52064632 0.58078995 ... 0.42549372 0.37701975 0.43087971]\n",
      "[0.28258801 0.32483349 0.40285442 ... 0.22873454 0.29666984 0.37507136]\n",
      "[0.45492487 0.57429048 0.45492487 ... 0.42654424 0.3263773  0.34307179]\n",
      "[0.37950863 0.22477784 0.24464192 ... 0.32096184 0.26346053 0.29168845]\n",
      "[0.3387873  0.34600577 0.4518768  ... 0.47930703 0.31568816 0.36381136]\n",
      "[0.82087099 0.76992605 0.0566968  ... 0.62284306 0.34182416 0.05094495]\n",
      "[0.45915735 0.46603611 0.5855546  ... 0.711092   0.34651763 0.51590714]\n",
      "[0.71333964 0.01608325 0.01513718 ... 0.0179754  0.01135289 0.859035  ]\n",
      "[0.16613076 0.1318328  0.03108253 ... 0.16613076 0.04180064 0.01500536]\n",
      "[0.2962963  0.3587963  0.14351852 ... 0.60185185 0.00925926 0.09259259]\n",
      "[0.55474453 0.53284672 0.22627737 ... 0.76642336 0.09489051 0.16058394]\n",
      "[0.63092979 0.48908918 0.14468691 ... 0.528463   0.11764706 0.14421252]\n",
      "[0.3539267  0.06626776 0.56993269 ... 0.51802543 0.2038893  0.55228123]\n",
      "[0.24115973 0.05927381 0.42202954 ... 0.32678499 0.20512126 0.4482455 ]\n",
      "[0.31239092 0.11518325 0.20593368 ... 0.21116928 0.02879581 0.495637  ]\n",
      "[0.26985491 0.10213822 0.34851088 ... 0.33342879 0.21687667 0.34526537]\n",
      "[0.24492159 0.06220398 0.36796127 ... 0.32301863 0.13251237 0.23102831]\n",
      "[0.57140182 0.1788557  0.60146081 ... 0.6463152  0.36988482 0.63976028]\n",
      "[0.47908903 0.0926156  0.4089717  ... 0.45162181 0.18247067 0.52726018]\n",
      "[0.22119645 0.09748892 0.29689808 ... 0.22119645 0.12370753 0.63183161]\n",
      "[0.43783422 0.43649733 0.78977273 ... 0.40040107 0.33923797 0.52540107]\n",
      "[0.38998211 0.28980322 0.27370304 ... 0.24060823 0.36404293 0.69677996]\n",
      "[0.29527991 0.1712404  0.44456641 ... 0.26893524 0.3194292  0.52030735]\n",
      "[0.53512706 0.3019432  0.25710015 ... 0.64723468 0.28101644 0.3064275 ]\n"
     ]
    }
   ],
   "source": [
    "# Create sequences for supervised learning\n",
    "def create_sequences(data, sequence_length):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - sequence_length):\n",
    "        X.append(data[i:i + sequence_length])\n",
    "        y.append(data[i + sequence_length])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "data = pd.read_csv(\"data/electricity_26000x100.csv\")\n",
    "# print(data)\n",
    "\n",
    "# Create directories for each model dataset\n",
    "for i in range(NUM_CLIENTS):\n",
    "    directory = f'models_data/model_{i}_data'\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "for i in range(NUM_CLIENTS):\n",
    "    X, y = create_sequences(data.loc[:, str(i)], SEQUENCE_LENGTH)\n",
    "\n",
    "    # Split into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "    # Scale the data (normalization)\n",
    "    scaler = preprocessing.MinMaxScaler()\n",
    "    X_train = scaler.fit_transform(X_train.reshape(-1, 1)).reshape(X_train.shape)\n",
    "    X_test = scaler.transform(X_test.reshape(-1, 1)).reshape(X_test.shape)\n",
    "    y_train = scaler.fit_transform(y_train.reshape(-1, 1)).flatten()\n",
    "    y_test = scaler.transform(y_test.reshape(-1, 1)).flatten()\n",
    "    print(y_train)\n",
    "\n",
    "    # Save datasets\n",
    "    np.save(f'models_data/model_{i}_data/x_train.npy', X_train)\n",
    "    np.save(f'models_data/model_{i}_data/y_train.npy', y_train)\n",
    "    np.save(f'models_data/model_{i}_data/x_test.npy', X_test)\n",
    "    np.save(f'models_data/model_{i}_data/y_test.npy', y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Define the Neural Network\n",
    "We define a neural network with two hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network\n",
    "class ComplexNet(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(ComplexNet, self).__init__()\n",
    "        self.hidden1 = nn.Linear(input_size, 10)\n",
    "        self.bn1 = nn.BatchNorm1d(10)\n",
    "        self.hidden2 = nn.Linear(10, 5)\n",
    "        self.bn2 = nn.BatchNorm1d(5)\n",
    "        self.output = nn.Linear(5, 1)\n",
    "        \n",
    "        # Initialize weights\n",
    "        nn.init.kaiming_uniform_(self.hidden1.weight, nonlinearity='relu')\n",
    "        nn.init.kaiming_uniform_(self.hidden2.weight, nonlinearity='relu')\n",
    "        nn.init.xavier_uniform_(self.output.weight)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.bn1(self.hidden1(x)))\n",
    "        x = torch.relu(self.bn2(self.hidden2(x)))\n",
    "        x = self.output(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alexnet_regression(input_shape):\n",
    "    model = tf.keras.Sequential([\n",
    "        # Input Layer\n",
    "        tf.keras.layers.InputLayer(input_shape=input_shape[1:]),\n",
    "\n",
    "        # Fully connected layers for smaller inputs\n",
    "        tf.keras.layers.Dense(4096, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(2048, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(512, activation='relu'),\n",
    "\n",
    "        # Regression Output\n",
    "        tf.keras.layers.Dense(1)  # Single unit for regression output\n",
    "    ])\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "                  loss='mean_squared_error',\n",
    "                  metrics=['mae'])  # Regression loss and metric\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Train Multiple Models Independently and Save Weights\n",
    "We instantiate and train `NUM_MODELS` separate models using the corresponding datasets. During training, we save the weights of each model at every training round to a separate CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train a single model and save weights\n",
    "def train_model(model_id):\n",
    "    # Load dataset for the model\n",
    "    directory = f'model_{model_id}_data'\n",
    "    x_train = np.load(os.path.join('models_data', directory, 'x_train.npy'))\n",
    "    y_train = np.load(os.path.join('models_data', directory, 'y_train.npy'))\n",
    "    x_train_tensor = torch.tensor(x_train, dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "    # model = alexnet_regression(input_shape=(SEQUENCE_LENGTH, 1))\n",
    "    # model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "    model = ComplexNet(input_size=SEQUENCE_LENGTH)\n",
    "    criterion = lambda output, target: torch.sqrt(nn.MSELoss()(output, target))\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "    # model = alexnet_regression(input_shape)\n",
    "    # # Compile the model\n",
    "    # model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "    # Train the model\n",
    "    # history = model.fit(X_train, y_train, epochs=EPOCHS, batch_size=32, validation_split=0.2, verbose=1)\n",
    "\n",
    "    # Evaluate the model\n",
    "    # loss, mae = model.evaluate(X_test, y_test, verbose=0)\n",
    "    # print(f\"Test Loss: {loss:.4f}, Test MAE: {mae:.4f}\")\n",
    "\n",
    "    # Predict on the test set\n",
    "    # y_pred = model.predict(X_test)\n",
    "\n",
    "    # Rescale predictions and true values\n",
    "    # y_pred_rescaled = scaler.inverse_transform(y_pred)\n",
    "    # y_test_rescaled = scaler.inverse_transform(y_test.reshape(-1, 1))\n",
    "\n",
    "    # Plot predictions vs true values\n",
    "    # plt.figure(figsize=(10, 6))\n",
    "    # plt.plot(y_test_rescaled, label=\"True Values\", color=\"blue\")\n",
    "    # plt.plot(y_pred_rescaled, label=\"Predictions\", color=\"orange\")\n",
    "    # plt.title(\"True vs Predicted Values (AlexNet for Time Series Regression)\")\n",
    "    # plt.xlabel(\"Time Step\")\n",
    "    # plt.ylabel(\"Value\")\n",
    "    # plt.legend()\n",
    "    # plt.grid(True)\n",
    "    # plt.show()\n",
    "\n",
    "    csv_filename = f'weights_tracking_models/weights_tracking_model_{model_id}.csv'\n",
    "    training_losses = []\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        # Forward pass\n",
    "        outputs = model(x_train_tensor)\n",
    "        loss = criterion(outputs, y_train_tensor)\n",
    "        training_losses.append(loss.item())\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        # Save weights at intervals\n",
    "        if epoch % SAVE_INTERVAL == 0:\n",
    "            # Extract weights and flatten them\n",
    "            weights_list = []\n",
    "            for name, param in model.named_parameters():\n",
    "                if param.requires_grad:\n",
    "                    weights_list.extend(param.data.view(-1).tolist())\n",
    "\n",
    "            # Check if CSV file exists\n",
    "            file_exists = os.path.isfile(csv_filename)\n",
    "\n",
    "            # Open CSV file to save weights\n",
    "            with open(csv_filename, mode='a', newline='') as file:\n",
    "                writer = csv.writer(file)\n",
    "\n",
    "                # If file doesn't exist, write the header\n",
    "                if not file_exists:\n",
    "                    header = []\n",
    "                    for name, param in model.named_parameters():\n",
    "                        if param.requires_grad:\n",
    "                            num_weights = param.data.numel()\n",
    "                            header.extend([f'{name}_weight_{i}' for i in range(num_weights)])\n",
    "                    writer.writerow(header)\n",
    "\n",
    "                # Write weights to CSV\n",
    "                writer.writerow(weights_list)\n",
    "\n",
    "    # Plot training loss\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(range(1, EPOCHS + 1), training_losses, label=f'Model {model_id} Training Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.title(f'Training Loss for Model {model_id}')\n",
    "    plt.show()\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# Train NUM_MODELS models independently and save their weights\n",
    "models = [train_model(i) for i in range(NUM_CLIENTS)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Federated learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client data:  100\n",
      "Server update, mean_client_weights:  ModelWeights(trainable=(<tf.Tensor 'mean_client_weights:0' shape=(20, 64) dtype=float32>, <tf.Tensor 'mean_client_weights_1:0' shape=(64,) dtype=float32>, <tf.Tensor 'mean_client_weights_2:0' shape=(64, 32) dtype=float32>, <tf.Tensor 'mean_client_weights_3:0' shape=(32,) dtype=float32>, <tf.Tensor 'mean_client_weights_4:0' shape=(32, 1) dtype=float32>, <tf.Tensor 'mean_client_weights_5:0' shape=(1,) dtype=float32>), non_trainable=())\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "There is no such attribute 'trainable' in this federated tuple. Valid attributes: ()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 201\u001b[0m\n\u001b[1;32m    197\u001b[0m   client_optimizer \u001b[38;5;241m=\u001b[39m tff\u001b[38;5;241m.\u001b[39mlearning\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mbuild_adamw(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m)\n\u001b[1;32m    198\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m client_update(tff_model, tf_dataset, server_weights, client_optimizer)\n\u001b[1;32m    200\u001b[0m \u001b[38;5;129;43m@tff\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfederated_computation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfederated_server_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfederated_dataset_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m--> 201\u001b[0m \u001b[38;5;28;43;01mdef\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43mnext_fn\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mserver_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfederated_dataset\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Broadcast the server weights to the clients.\u001b[39;49;00m\n\u001b[1;32m    203\u001b[0m \u001b[43m  \u001b[49m\u001b[43mserver_weights_at_client\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtff\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfederated_broadcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mserver_weights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Each client computes their updated weights.\u001b[39;49;00m\n",
      "File \u001b[0;32m/mnt/d/Research/FL/OfficialProject/Codebase/.venv39/lib/python3.9/site-packages/tensorflow_federated/python/core/impl/computation/computation_wrapper.py:511\u001b[0m, in \u001b[0;36mComputationWrapper.__call__.<locals>.<lambda>\u001b[0;34m(fn)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m   provided_types \u001b[38;5;241m=\u001b[39m _to_types(args)\n\u001b[0;32m--> 511\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mlambda\u001b[39;00m fn: \u001b[43m_wrap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    512\u001b[0m \u001b[43m      \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wrapper_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprovided_types\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_infer_type_fn\u001b[49m\n\u001b[1;32m    513\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/d/Research/FL/OfficialProject/Codebase/.venv39/lib/python3.9/site-packages/tensorflow_federated/python/core/impl/computation/computation_wrapper.py:236\u001b[0m, in \u001b[0;36m_wrap\u001b[0;34m(fn, wrapper_fn, parameter_types, infer_type_fn)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    234\u001b[0m   \u001b[38;5;66;03m# Either we have a concrete parameter type, or this is no-arg function.\u001b[39;00m\n\u001b[1;32m    235\u001b[0m   parameter_type \u001b[38;5;241m=\u001b[39m _parameter_type(parameters, parameter_types)\n\u001b[0;32m--> 236\u001b[0m   wrapped_fn \u001b[38;5;241m=\u001b[39m \u001b[43m_wrap_concrete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwrapper_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameter_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;66;03m# When applying a decorator, the __doc__ attribute with the documentation\u001b[39;00m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;66;03m# in triple-quotes is not automatically transferred from the function on\u001b[39;00m\n\u001b[1;32m    240\u001b[0m wrapped_fn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__doc__\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(fn, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__doc__\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m/mnt/d/Research/FL/OfficialProject/Codebase/.venv39/lib/python3.9/site-packages/tensorflow_federated/python/core/impl/computation/computation_wrapper.py:98\u001b[0m, in \u001b[0;36m_wrap_concrete\u001b[0;34m(fn, wrapper_fn, parameter_type)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[1;32m     96\u001b[0m   name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 98\u001b[0m concrete_fn \u001b[38;5;241m=\u001b[39m \u001b[43mwrapper_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameter_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munpack\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m py_typecheck\u001b[38;5;241m.\u001b[39mcheck_type(\n\u001b[1;32m    100\u001b[0m     concrete_fn,\n\u001b[1;32m    101\u001b[0m     computation_impl\u001b[38;5;241m.\u001b[39mConcreteComputation,\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalue returned by the wrapper\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    103\u001b[0m )\n\u001b[1;32m    104\u001b[0m result_parameter_type \u001b[38;5;241m=\u001b[39m concrete_fn\u001b[38;5;241m.\u001b[39mtype_signature\u001b[38;5;241m.\u001b[39mparameter\n",
      "File \u001b[0;32m/mnt/d/Research/FL/OfficialProject/Codebase/.venv39/lib/python3.9/site-packages/tensorflow_federated/python/core/impl/federated_context/federated_computation.py:43\u001b[0m, in \u001b[0;36m_federated_computation_wrapper_fn\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m     38\u001b[0m fn \u001b[38;5;241m=\u001b[39m function_utils\u001b[38;5;241m.\u001b[39mwrap_as_zero_or_one_arg_callable(\n\u001b[1;32m     39\u001b[0m     fn, parameter_type, unpack\n\u001b[1;32m     40\u001b[0m )\n\u001b[1;32m     41\u001b[0m context_stack \u001b[38;5;241m=\u001b[39m context_stack_impl\u001b[38;5;241m.\u001b[39mcontext_stack\n\u001b[1;32m     42\u001b[0m target_lambda, extra_type_spec \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m---> 43\u001b[0m     \u001b[43mfederated_computation_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_or_one_arg_fn_to_building_block\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameter_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameter_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontext_stack\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m        \u001b[49m\u001b[43msuggested_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m )\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m computation_impl\u001b[38;5;241m.\u001b[39mConcreteComputation(\n\u001b[1;32m     52\u001b[0m     computation_proto\u001b[38;5;241m=\u001b[39mtarget_lambda\u001b[38;5;241m.\u001b[39mproto,\n\u001b[1;32m     53\u001b[0m     context_stack\u001b[38;5;241m=\u001b[39mcontext_stack,\n\u001b[1;32m     54\u001b[0m     annotated_type\u001b[38;5;241m=\u001b[39mextra_type_spec,\n\u001b[1;32m     55\u001b[0m )\n",
      "File \u001b[0;32m/mnt/d/Research/FL/OfficialProject/Codebase/.venv39/lib/python3.9/site-packages/tensorflow_federated/python/core/impl/federated_context/federated_computation_utils.py:76\u001b[0m, in \u001b[0;36mzero_or_one_arg_fn_to_building_block\u001b[0;34m(fn, parameter_name, parameter_type, context_stack, suggested_name)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context_stack\u001b[38;5;241m.\u001b[39minstall(context):\n\u001b[1;32m     75\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m parameter_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 76\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalue_impl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mValue\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbuilding_blocks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mReference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparameter_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameter_type\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     82\u001b[0m     result \u001b[38;5;241m=\u001b[39m fn()\n",
      "File \u001b[0;32m/mnt/d/Research/FL/OfficialProject/Codebase/.venv39/lib/python3.9/site-packages/tensorflow_federated/python/core/impl/computation/function_utils.py:464\u001b[0m, in \u001b[0;36mwrap_as_zero_or_one_arg_callable.<locals>.<lambda>\u001b[0;34m(arg)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNameError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    463\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mArgs to be bound must be in scope.\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m--> 464\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mlambda\u001b[39;00m arg: \u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameter_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munpack\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/d/Research/FL/OfficialProject/Codebase/.venv39/lib/python3.9/site-packages/tensorflow_federated/python/core/impl/computation/function_utils.py:457\u001b[0m, in \u001b[0;36mwrap_as_zero_or_one_arg_callable.<locals>._call\u001b[0;34m(fn, parameter_type, arg, unpack)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call\u001b[39m(fn, parameter_type, arg, unpack):\n\u001b[1;32m    456\u001b[0m   args, kwargs \u001b[38;5;241m=\u001b[39m unpack_arg(fn, parameter_type, arg, unpack)\n\u001b[0;32m--> 457\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[49], line 212\u001b[0m, in \u001b[0;36mnext_fn\u001b[0;34m(server_weights, federated_dataset)\u001b[0m\n\u001b[1;32m    206\u001b[0m client_weights \u001b[38;5;241m=\u001b[39m tff\u001b[38;5;241m.\u001b[39mfederated_map(\n\u001b[1;32m    207\u001b[0m     client_update_fn, (federated_dataset, server_weights_at_client)\n\u001b[1;32m    208\u001b[0m )\n\u001b[1;32m    210\u001b[0m weight_denom \u001b[38;5;241m=\u001b[39m client_weights\u001b[38;5;241m.\u001b[39mtrainable\n\u001b[0;32m--> 212\u001b[0m tf\u001b[38;5;241m.\u001b[39mprint(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClient weights: \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mweight_denom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainable\u001b[49m)\n\u001b[1;32m    214\u001b[0m \u001b[38;5;66;03m# The server averages these updates.\u001b[39;00m\n\u001b[1;32m    215\u001b[0m mean_client_weights \u001b[38;5;241m=\u001b[39m tff\u001b[38;5;241m.\u001b[39mfederated_mean(client_weights)\n",
      "File \u001b[0;32m/mnt/d/Research/FL/OfficialProject/Codebase/.venv39/lib/python3.9/site-packages/tensorflow_federated/python/core/impl/federated_context/value_impl.py:126\u001b[0m, in \u001b[0;36mValue.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_federated_struct(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtype_signature):\n\u001b[1;32m    125\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m structure\u001b[38;5;241m.\u001b[39mname_list(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtype_signature\u001b[38;5;241m.\u001b[39mmember):  \u001b[38;5;66;03m# pytype: disable=attribute-error\u001b[39;00m\n\u001b[0;32m--> 126\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m    127\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThere is no such attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m in this federated tuple. Valid \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    128\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattributes: (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    129\u001b[0m             name, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mdir\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtype_signature\u001b[38;5;241m.\u001b[39mmember))  \u001b[38;5;66;03m# pytype: disable=attribute-error\u001b[39;00m\n\u001b[1;32m    130\u001b[0m         )\n\u001b[1;32m    131\u001b[0m     )\n\u001b[1;32m    133\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m Value(\n\u001b[1;32m    134\u001b[0m       building_block_factory\u001b[38;5;241m.\u001b[39mcreate_federated_getattr_call(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_comp, name)\n\u001b[1;32m    135\u001b[0m   )\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mdir\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtype_signature):\n",
      "\u001b[0;31mAttributeError\u001b[0m: There is no such attribute 'trainable' in this federated tuple. Valid attributes: ()"
     ]
    }
   ],
   "source": [
    "clients_data = []\n",
    "clients_test_data = []\n",
    "for cid in range(NUM_CLIENTS):\n",
    "    directory = f'model_{cid}_data'\n",
    "    x_train = np.load(os.path.join('models_data', directory, 'x_train.npy'))\n",
    "    y_train = np.load(os.path.join('models_data', directory, 'y_train.npy')).reshape(-1, 1)\n",
    "\n",
    "    x_test = np.load(os.path.join('models_data', directory, 'x_test.npy'))\n",
    "    y_test = np.load(os.path.join('models_data', directory, 'y_test.npy')).reshape(-1, 1)\n",
    "\n",
    "    clients_data.append((x_train, y_train))\n",
    "    clients_test_data.append((x_test, y_test))\n",
    "\n",
    "def get_client_train_data(cid):\n",
    "  directory = f'model_{cid}_data'\n",
    "  x_train = np.load(os.path.join('models_data', directory, 'x_train.npy'))\n",
    "  y_train = np.load(os.path.join('models_data', directory, 'y_train.npy')).reshape(-1, 1)\n",
    "  return (x_train, y_train)\n",
    "\n",
    "def get_client_test_data(cid):\n",
    "  directory = f'model_{cid}_data'\n",
    "  x_test = np.load(os.path.join('models_data', directory, 'x_test.npy'))\n",
    "  y_test = np.load(os.path.join('models_data', directory, 'y_test.npy')).reshape(-1, 1)\n",
    "  return (x_test, y_test)\n",
    "\n",
    "# federated_data = tf.data.Dataset.from_tensor_slices(clients_data).batch(20)\n",
    "# federated_test_data = tf.data.Dataset.from_tensor_slices(clients_test_data).batch(20)\n",
    "\n",
    "# Preprocess data for TFF\n",
    "def preprocess_client_data(client_data):\n",
    "    def create_tf_dataset(X, y):\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((X, y))\n",
    "        return dataset.batch(10)  # Batch size of 10\n",
    "    return [create_tf_dataset(X, y) for X, y in client_data]\n",
    "\n",
    "def preprocess(data):\n",
    "  #  print(data)\n",
    "   dataset = tf.data.Dataset.from_tensor_slices(data)\n",
    "   return dataset.batch(100)\n",
    "\n",
    "federated_data = [preprocess(get_client_train_data(i)) for i in range(NUM_CLIENTS)] #preprocess_client_data(clients_data)\n",
    "federated_test_data = [preprocess(get_client_test_data(i)) for i in range(NUM_CLIENTS)] #preprocess_client_data(clients_test_data)\n",
    "\n",
    "print(\"Client data: \", len(federated_data))\n",
    "# print(\"Client data shape: \", federated_data.shape)\n",
    "\n",
    "# def preprocess(dataset):\n",
    "\n",
    "#   def batch_format_fn(element):\n",
    "#     \"\"\"Flatten a batch of EMNIST data and return a (features, label) tuple.\"\"\"\n",
    "#     return (tf.reshape(element['pixels'], [-1, 784]),\n",
    "#             tf.reshape(element['label'], [-1, 1]))\n",
    "\n",
    "#   return dataset.batch(20).map(batch_format_fn)\n",
    "\n",
    "# class NumpyClientData(tff.simulation.datasets.ClientData):\n",
    "#     def __init__(self, data):\n",
    "#         self._data = data\n",
    "#         self._client_ids = list(data.keys())\n",
    "\n",
    "#     @property\n",
    "#     def client_ids(self):\n",
    "#         return self._client_ids\n",
    "\n",
    "#     def create_tf_dataset_for_client(self, client_id):\n",
    "#         # Extract numpy arrays\n",
    "#         X, y = self._data[client_id]\n",
    "#         # Wrap in tf.data.Dataset\n",
    "#         return tf.data.Dataset.from_tensor_slices((X, y)).batch(20)\n",
    "\n",
    "# clients_data = {}\n",
    "# clients_test_data = {}\n",
    "# for cid in range(NUM_CLIENTS):\n",
    "#   X_train = np.load(os.path.join('models_data', directory, 'x_train.npy'))\n",
    "#   y_train = np.load(os.path.join('models_data', directory, 'y_train.npy')).reshape(-1, 1)\n",
    "#   clients_data[str(cid)] = (X_train, y_train)\n",
    "\n",
    "#   X_test = np.load(os.path.join('models_data', directory, 'x_test.npy'))\n",
    "#   y_train = np.load(os.path.join('models_data', directory, 'y_test.npy')).reshape(-1, 1)\n",
    "#   clients_test_data[str(cid)] = (X_test, y_test)\n",
    "\n",
    "# federated_data = NumpyClientData(clients_data)\n",
    "# federated_data_test = NumpyClientData(clients_test_data)\n",
    "\n",
    "# federated_train_data = [preprocess(federated_data.create_tf_dataset_for_client(str(x)))\n",
    "#   for x in range(NUM_CLIENTS)\n",
    "# ]\n",
    "\n",
    "# federated_test_data = [preprocess(federated_data_test.create_tf_dataset_for_client(str(x)))\n",
    "#   for x in range(NUM_CLIENTS)\n",
    "# ]\n",
    "\n",
    "def convert_spec_to_tff_type(tensor_spec):\n",
    "    # print(\"--------- Tensor spec: {}\".format(tensor_spec))\n",
    "    # print(\"--------- Dummy tensor spec: {} and {}\".format(repr(tensor_spec.dtype), tensor_spec.shape))\n",
    "\n",
    "    return tff.TensorType(dtype=tensor_spec.dtype.as_numpy_dtype, shape=tensor_spec.shape)\n",
    "\n",
    "# Define the model\n",
    "def create_regression_model(input_dim):\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Dense(64, activation='relu', input_shape=(input_dim,)),\n",
    "        tf.keras.layers.Dense(32, activation='relu'),\n",
    "        tf.keras.layers.Dense(1)  # Regression output\n",
    "\n",
    "        # tf.keras.layers.Input(shape=(784,)),\n",
    "        # tf.keras.layers.Dense(10, kernel_initializer=initializer),\n",
    "        # tf.keras.layers.Softmax(),\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Wrap the Keras model for TFF\n",
    "def model_fn():\n",
    "    keras_model = create_regression_model(20)\n",
    "    return tff.learning.models.from_keras_model(\n",
    "        keras_model=keras_model,\n",
    "        input_spec=federated_data[0].element_spec,\n",
    "        loss_fn=tf.keras.losses.MeanSquaredError(),\n",
    "        metrics=[tf.keras.metrics.MeanSquaredError()]\n",
    "    )\n",
    "\n",
    "tff_model = tff.learning.models.functional_model_from_keras(\n",
    "        keras_model=create_regression_model(20),\n",
    "        input_spec=federated_data[0].element_spec,\n",
    "        loss_fn=tf.keras.losses.MeanSquaredError(),\n",
    "        metrics_constructor=collections.OrderedDict(\n",
    "           accuracy=tf.keras.metrics.MeanSquaredError\n",
    "        )\n",
    "    )\n",
    "\n",
    "@tff.tensorflow.computation\n",
    "def server_init():\n",
    "  return tff.learning.models.ModelWeights(*tff_model.initial_weights)\n",
    "\n",
    "tf_dataset_type = tff.SequenceType(\n",
    "    # tff.types.to_type(tff_model.input_spec)\n",
    "    [convert_spec_to_tff_type(spec) for spec in tff_model.input_spec]\n",
    ")\n",
    "model_weights_type = server_init.type_signature.result\n",
    "\n",
    "federated_server_type = tff.FederatedType(model_weights_type, tff.SERVER)\n",
    "federated_dataset_type = tff.FederatedType(tf_dataset_type, tff.CLIENTS)\n",
    "\n",
    "@tff.federated_computation\n",
    "def initialize_fn():\n",
    "  return tff.federated_eval(server_init, tff.SERVER)\n",
    "\n",
    "@tf.function\n",
    "def client_update(model, dataset, initial_weights, client_optimizer):\n",
    "  \"\"\"Performs training (using the server model weights) on the client's dataset.\"\"\"\n",
    "  # Initialize the client model with the current server weights and the optimizer\n",
    "  # state.\n",
    "  client_weights = initial_weights.trainable\n",
    "  optimizer_state = client_optimizer.initialize(\n",
    "      tf.nest.map_structure(tf.TensorSpec.from_tensor, client_weights)\n",
    "  )\n",
    "\n",
    "  # Use the client_optimizer to update the local model.\n",
    "  for batch in dataset:\n",
    "    # tf.print(\"Training local epoch: \", i)\n",
    "    x, y = batch\n",
    "    # print(\"x, y: \", x, y)\n",
    "    with tf.GradientTape() as tape:\n",
    "      tape.watch(client_weights)\n",
    "      # Compute a forward pass on the batch of data\n",
    "      outputs = model.predict_on_batch(\n",
    "          model_weights=(client_weights, ()), x=x, training=True\n",
    "      )\n",
    "      loss = model.loss(output=outputs, label=y)\n",
    "\n",
    "    # Compute the corresponding gradient\n",
    "    grads = tape.gradient(loss, client_weights)\n",
    "\n",
    "    # Apply the gradient using a client optimizer.\n",
    "    optimizer_state, client_weights = client_optimizer.next(\n",
    "        optimizer_state, weights=client_weights, gradients=grads\n",
    "    )\n",
    "\n",
    "    tf.print(\"Weights of client: \", len(client_weights[0][1]))\n",
    "    \n",
    "\n",
    "  return tff.learning.models.ModelWeights(client_weights, non_trainable=())\n",
    "\n",
    "@tf.function\n",
    "def server_update(model, mean_client_weights):\n",
    "  \"\"\"Updates the server model weights as the average of the client model weights.\"\"\"\n",
    "  del model  # Unused, just take the mean_client_weights.\n",
    "  print(\"Server update, mean_client_weights: \", mean_client_weights)\n",
    "  return mean_client_weights\n",
    "\n",
    "@tff.tensorflow.computation(model_weights_type)\n",
    "def server_update_fn(mean_client_weights):\n",
    "  return server_update(tff_model, mean_client_weights)\n",
    "\n",
    "@tff.tensorflow.computation(tf_dataset_type, model_weights_type)\n",
    "def client_update_fn(tf_dataset, server_weights):\n",
    "  client_optimizer = tff.learning.optimizers.build_adamw(learning_rate=0.01)\n",
    "  return client_update(tff_model, tf_dataset, server_weights, client_optimizer)\n",
    "\n",
    "@tff.federated_computation(federated_server_type, federated_dataset_type)\n",
    "def next_fn(server_weights, federated_dataset):\n",
    "  # Broadcast the server weights to the clients.\n",
    "  server_weights_at_client = tff.federated_broadcast(server_weights)\n",
    "\n",
    "  # Each client computes their updated weights.\n",
    "  client_weights = tff.federated_map(\n",
    "      client_update_fn, (federated_dataset, server_weights_at_client)\n",
    "  )\n",
    "\n",
    "  weight_denom = client_weights.trainable\n",
    "\n",
    "  tf.print(\"Client weights: \", weight_denom)\n",
    "\n",
    "  # The server averages these updates.\n",
    "  mean_client_weights = tff.federated_mean(client_weights)\n",
    "\n",
    "  tf.print(\"Mean client weights: \", mean_client_weights)\n",
    "\n",
    "  # The server updates its model.\n",
    "  server_weights = tff.federated_map(server_update_fn, mean_client_weights)\n",
    "\n",
    "  tf.print(\"Server weights: \", server_weights)\n",
    "\n",
    "  return server_weights\n",
    "\n",
    "federated_algorithm = tff.templates.IterativeProcess(\n",
    "    initialize_fn=initialize_fn,\n",
    "    next_fn=next_fn\n",
    ")\n",
    "\n",
    "def evaluate(model_weights):\n",
    "  keras_model = create_regression_model(20)\n",
    "  keras_model.compile(\n",
    "      loss=tf.keras.losses.MeanSquaredError(),\n",
    "      metrics=[tf.keras.metrics.MeanAbsoluteError()],\n",
    "  )\n",
    "  model_weights.assign_weights_to(keras_model)\n",
    "  return keras_model.evaluate(federated_test_data[1])\n",
    "\n",
    "server_state = federated_algorithm.initialize()\n",
    "\n",
    "# print(federated_test_data)\n",
    "# server_state\n",
    "evaluate(server_state)\n",
    "\n",
    "loss_history = []\n",
    "for _ in range(5):\n",
    "  server_state = federated_algorithm.next(server_state, federated_data)\n",
    "  tf.print(\"Server state: \", len(server_state.trainable[0][1]))\n",
    "  loss, mea = evaluate(server_state)\n",
    "  loss_history.append(loss)\n",
    "\n",
    "# Plot loss history\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, 6), loss_history, label='Training Loss')\n",
    "plt.title(\"Federated Learning: Training Loss Across Rounds\")\n",
    "plt.xlabel(\"Round\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_keras_model():\n",
    "  initializer = tf.keras.initializers.GlorotNormal(seed=0)\n",
    "  return tf.keras.models.Sequential([\n",
    "      tf.keras.layers.Input(shape=(784,)),\n",
    "      tf.keras.layers.Dense(10, kernel_initializer=initializer),\n",
    "      tf.keras.layers.Softmax(),\n",
    "  ])\n",
    "\n",
    "keras_model = create_keras_model()\n",
    "tff_model = tff.learning.models.functional_model_from_keras(\n",
    "    keras_model,\n",
    "    loss_fn=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "    input_spec=clients_data[0].element_spec,\n",
    "    metrics_constructor=collections.OrderedDict(\n",
    "        accuracy=tf.keras.metrics.SparseCategoricalAccuracy\n",
    "    ),\n",
    ")\n",
    "tff_model = tff.learning.models.functional_model_from_keras(\n",
    "   keras_model,\n",
    "   loss_fn=tf.keras.losses.MeanSquaredError(),\n",
    "   input_spec=clients_data[0].element_spec,\n",
    "   metrics_constructor=collections.OrderedDict(\n",
    "      accuracy=tf.keras.metrics.MeanAbsoluteError\n",
    "   ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clients_data = []\n",
    "for cid in range(NUM_CLIENTS):\n",
    "    directory = f'model_{cid}_data'\n",
    "    x_train = np.load(os.path.join('models_data', directory, 'x_train.npy'))\n",
    "    y_train = np.load(os.path.join('models_data', directory, 'y_train.npy')).reshape(-1, 1)\n",
    "\n",
    "    clients_data.append((x_train, y_train))\n",
    "    # x_train_tensor = torch.tensor(x_train, dtype=torch.float32)\n",
    "    # y_train_tensor = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "# Generate synthetic data for federated clients\n",
    "# def create_synthetic_data(num_clients, num_samples, input_dim):\n",
    "#     client_data = []\n",
    "#     for i in range(num_clients):\n",
    "#         np.random.seed(i)\n",
    "#         X = np.random.rand(num_samples, input_dim)\n",
    "#         y = X.sum(axis=1) + np.random.normal(scale=0.1, size=num_samples)  # Simple regression target\n",
    "#         client_data.append((X, y))\n",
    "#     return client_data\n",
    "\n",
    "# Preprocess data for TFF\n",
    "def preprocess_client_data(client_data):\n",
    "    def create_tf_dataset(X, y):\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((X, y))\n",
    "        return dataset.batch(100)  # Batch size of 10\n",
    "    return [create_tf_dataset(X, y) for X, y in client_data]\n",
    "\n",
    "# Define the model\n",
    "def create_regression_model(input_dim):\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Dense(64, activation='relu', input_shape=(input_dim,)),\n",
    "        tf.keras.layers.Dense(32, activation='relu'),\n",
    "        tf.keras.layers.Dense(1)  # Regression output\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Wrap the Keras model for TFF\n",
    "def model_fn():\n",
    "    keras_model = create_regression_model(input_dim)\n",
    "    return tff.learning.models.from_keras_model(\n",
    "        keras_model=keras_model,\n",
    "        input_spec=clients_data[0].element_spec,\n",
    "        loss=tf.keras.losses.MeanSquaredError(),\n",
    "        metrics=[tf.keras.metrics.MeanSquaredError()]\n",
    "    )\n",
    "\n",
    "# Configuration\n",
    "num_clients = 5\n",
    "num_samples = 1000\n",
    "input_dim = 20\n",
    "# client_data = preprocess_client_data(create_synthetic_data(num_clients, num_samples, input_dim))\n",
    "clients_data = preprocess_client_data(clients_data)\n",
    "\n",
    "# Federated averaging process\n",
    "training_process = tff.learning.algorithms.build_weighted_fed_avg(\n",
    "    model_fn=model_fn,\n",
    "    client_optimizer_fn=tff.learning.optimizers.build_adamw(learning_rate=0.01),#lambda: tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    server_optimizer_fn=tff.learning.optimizers.build_sgdm(learning_rate=1.0) #lambda: tf.keras.optimizers.SGD(learning_rate=1.0)\n",
    ")\n",
    "\n",
    "# Initialize the federated learning process\n",
    "state = training_process.initialize()\n",
    "\n",
    "# Training configuration\n",
    "NUM_ROUNDS = 100\n",
    "loss_history = []\n",
    "\n",
    "# client_metrics = {i: {'accuracy': [], 'loss': []} for i in range(num_clients)}\n",
    "\n",
    "# for round_num in range(1, NUM_ROUNDS + 1):\n",
    "#     print(f\"Round {round_num}...\")\n",
    "#     # Simulate client computation\n",
    "#     round_metrics = []\n",
    "#     for client_idx, client_ds in enumerate(client_data):\n",
    "#         keras_model = create_regression_model(input_dim=input_dim)\n",
    "#         keras_model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    \n",
    "#         keras_model.set_weights([np.array(w.numpy()) for w in state.model.trainable])\n",
    "#         history = keras_model.fit(client_ds, epochs=1, verbose=0)\n",
    "#         client_metrics[client_idx]['accuracy'].append(history.history['binary_accuracy'][-1])\n",
    "#         client_metrics[client_idx]['loss'].append(history.history['loss'][-1])\n",
    "#         round_metrics.append((client_idx, history.history))\n",
    "\n",
    "#     # Aggregate client updates\n",
    "#     state, metrics = training_process.next(state, client_data)\n",
    "\n",
    "# # Plot metrics\n",
    "# for client_idx in range(num_clients):\n",
    "#     plt.figure(figsize=(12, 5))\n",
    "    \n",
    "#     # Accuracy plot\n",
    "#     plt.subplot(1, 2, 1)\n",
    "#     plt.plot(client_metrics[client_idx]['accuracy'], label=f'Client {client_idx} Accuracy')\n",
    "#     plt.title(f\"Client {client_idx} Accuracy\")\n",
    "#     plt.xlabel(\"Round\")\n",
    "#     plt.ylabel(\"Accuracy\")\n",
    "#     plt.legend()\n",
    "    \n",
    "#     # Loss plot\n",
    "#     plt.subplot(1, 2, 2)\n",
    "#     plt.plot(client_metrics[client_idx]['loss'], label=f'Client {client_idx} Loss')\n",
    "#     plt.title(f\"Client {client_idx} Loss\")\n",
    "#     plt.xlabel(\"Round\")\n",
    "#     plt.ylabel(\"Loss\")\n",
    "#     plt.legend()\n",
    "    \n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "# Simulate federated learning rounds\n",
    "for round_num in range(1, NUM_ROUNDS + 1):\n",
    "    state, metrics = training_process.next(state, clients_data)\n",
    "    loss = metrics['client_work']['train']['loss']\n",
    "    loss_history.append(loss)\n",
    "    print(f\"Round {round_num}: Loss = {loss:.4f}\")\n",
    "\n",
    "# Retrieve final model weights\n",
    "final_model_weights = state.global_model_weights\n",
    "print(\"Final model weights: \", final_model_weights)\n",
    "\n",
    "# Save the final model\n",
    "# final_model = create_regression_model(input_dim)\n",
    "# final_model.set_weights(final_model_weights)\n",
    "# final_model.save(\"federated_regression_model.h5\")\n",
    "\n",
    "# print(\"Federated learning completed. Final model saved.\")\n",
    "\n",
    "# Plot loss history\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, NUM_ROUNDS + 1), loss_history, marker='o', label='Training Loss')\n",
    "plt.title(\"Federated Learning: Training Loss Across Rounds\")\n",
    "plt.xlabel(\"Round\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_YN_to_binary(array):\n",
    "    # Use numpy where to convert 'Y' to 1 and 'N' to 0\n",
    "    binary_array = np.where(array == 'Y', 1, 0)\n",
    "    return binary_array\n",
    "\n",
    "def average_previous_rows(data, mask):\n",
    "    result = data.copy()  # Copy the data array to store results\n",
    "    \n",
    "    for i in range(2, len(data)):  # Start from the third row (index 2)\n",
    "        if np.all(mask[i] == 0):  # Check if the entire row in the mask is 0\n",
    "            result[i] = (data[i-1] + data[i-2]) / 2  # Average the previous two rows\n",
    "    \n",
    "    return result\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
